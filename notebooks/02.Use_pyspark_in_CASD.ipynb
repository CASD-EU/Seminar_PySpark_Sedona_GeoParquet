{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02.Use pyspark in CASD\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "- how to install spark and pyspark in CASD\n",
    "- how to create a spark session\n",
    "- read a csv file\n",
    "- read a parquet file\n",
    "\n",
    "\n",
    "## 1. Install spark and pyspark in CASD\n",
    "\n",
    "As we explained before,\n",
    "- `Apache Spark` is a distributed computation framework written mostly in Scala and Java, runs in a JVM(Java Virtual Machine)\n",
    "- `PySpark` is a Python API for Apache Spark.\n",
    "- PySpark talks to Spark engine via `Py4J`. Your Python code → Py4J(serialized and sent to JVM) → Spark core executes it → results sent back to python\n",
    "\n",
    "> Spark framework installation is essential, without it, pyspark will never work.\n",
    "\n",
    "### 1.1 Install spark framework\n",
    "\n",
    "CASD provides an installation script(`InstallSpark.ps1`) to install the `latest spark framework` and underlying JDK available in CASD.\n",
    "You can find this script in `Bureau->Raccourcis->Spark`.\n",
    "\n",
    "Open a powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# goto the target folder\n",
    "cd C:\\Users\\Public\\Desktop\\Raccourcis\\Spark\n",
    "\n",
    "# run the installation script\n",
    ".\\InstallSpark.ps1\n",
    "```\n",
    "\n",
    "> If everything works well, this script will install spark in `C:\\Users\\<your-id>\\AppData\\Local\\spark\\spark-3.5.5-bin-hadoop3`. It will also install open-jdk, winutils, and set up your\n",
    "> windows env vars.\n",
    "\n",
    "Now let's check if your spark works or not. Open a new powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# check the installed spark version\n",
    "spark-shell --version\n",
    "\n",
    "## it may take few seconds to show the output, be patient\n",
    "# expected output\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.2\n",
    "Branch HEAD\n",
    "Compiled by user ubuntu on 2024-08-06T11:36:15Z\n",
    "Revision bb7846dd487f259994fdc69e18e03382e3f64f42\n",
    "Url https://github.com/apache/spark\n",
    "Type --help for more information.\n",
    "```\n",
    "\n",
    "The\n",
    "> If you can't see the spark output, contact `service@casd.eu`\n",
    "\n",
    "### 1.2 Install pyspark\n",
    "\n",
    "As we mentioned before, CASD recommends you to create a `python virtual environment` for each for your python project.\n",
    "\n",
    "Suppose we will start a new project called `docs_in_paris`, let's create a python virtual environment with this name\n",
    "\n",
    "\n",
    "```powershell\n",
    "# create a python virtual environment\n",
    "conda create --name docs_in_paris python --offline\n",
    "\n",
    "# activate the virtual environment\n",
    "conda activate docs_in_paris\n",
    "\n",
    "# check python version\n",
    "python -V\n",
    "\n",
    "# check installed packages\n",
    "pip list\n",
    "\n",
    "# install pyspark\n",
    "pip install pyspark==3.5.5\n",
    "\n",
    "# check the installed pyspark version\n",
    "pip show pyspark\n",
    "\n",
    "# expected output\n",
    "Name: pyspark\n",
    "Version: 3.5.5\n",
    "Summary: Apache Spark Python API\n",
    "......\n",
    "```\n",
    "\n",
    "> You can notice that we have installed a specific version of pyspark. Because the pyspark version must be the same as the spark framework version. As the output in `section 1.1` is **spark-3.5.5**. So we need to\n",
    "> install pyspark-3.5.5\n",
    "\n",
    "## 2. Create a spark session\n",
    "\n",
    "A `Spark session` is the entry point to Apache Spark. A spark session allows us to interact with Spark’s core engine, no matter if you’re working in Python (PySpark), Scala, Java, or R.\n",
    "\n",
    "\n",
    "It encapsulates:\n",
    "\n",
    "- Cluster connection (or local JVM if local mode)\n",
    "- Configuration settings (memory, partitions, serializer, etc.)\n",
    "- Access to Spark’s APIs: Spark SQL API, DataFrame and Dataset API, RDD API (via .sparkContext), Streaming and machine learning APIs\n",
    "\n",
    "To create a spark session, you need to\n",
    "- import the required module\n",
    "- configure the spark session settings\n",
    "- create the spark session instance\n",
    "\n",
    "### 2.1 A minimum spark session creation\n",
    "\n",
    "Below shows a minimum spark session creation."
   ],
   "id": "b42cf971a64f18ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:26:35.181334Z",
     "start_time": "2025-08-12T13:26:35.003428Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession, DataFrame",
   "id": "6fbe633fa74cd51c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:02:57.154750Z",
     "start_time": "2025-08-12T13:02:52.478638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Use_pyspark_in_CASD\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "dbcc286d4b68fd6d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:07:23.974146Z",
     "start_time": "2025-08-12T13:07:23.920835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you can get and set configuration of your spark session any moments\n",
    "# get all conf\n",
    "spark.sparkContext.getConf().getAll()"
   ],
   "id": "45405f8a15ae8cea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.app.id', 'local-1755004036468'),\n",
       " ('spark.app.startTime', '1755004035248'),\n",
       " ('spark.kryoserializer.buffer.max', '512m'),\n",
       " ('spark.memory.fraction', '0.7'),\n",
       " ('spark.driver.host', 'LT-5CG4181HBL.casd.me'),\n",
       " ('spark.sql.shuffle.partitions', '12'),\n",
       " ('spark.serializer', 'org.apache.spark.serializer.KryoSerializer'),\n",
       " ('spark.driver.port', '63986'),\n",
       " ('spark.app.submitTime', '1755004035007'),\n",
       " ('spark.app.name', 'LocalMode_memo_config'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.memory.storageFraction', '0.3'),\n",
       " ('spark.reducer.maxSizeInFlight', '48m'),\n",
       " ('spark.shuffle.compress', 'true'),\n",
       " ('spark.sql.files.maxPartitionBytes', '128m'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError'),\n",
       " ('spark.driver.memory', '16g'),\n",
       " ('spark.kryoserializer.buffer', '64m'),\n",
       " ('spark.shuffle.spill.compress', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.driver.maxResultSize', '4g'),\n",
       " ('spark.shuffle.spill', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T12:35:33.534483Z",
     "start_time": "2025-08-12T12:35:18.780085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType\n",
    "\n",
    "# create a dataframe by using List\n",
    "dept = [(\"Alice\", \"Finance\", 10),\n",
    "        (\"Bob\", \"Marketing\", 20),\n",
    "        (\"Charlie\", \"Sales\", 30),\n",
    "        (\"Toto\", \"IT\", 40)\n",
    "        ]\n",
    "\n",
    "# give an explicit schema\n",
    "deptSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# create dataframe\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema=deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ],
   "id": "f32a5bef0a3d4ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---------+---+\n",
      "|name   |dept_name|age|\n",
      "+-------+---------+---+\n",
      "|Alice  |Finance  |10 |\n",
      "|Bob    |Marketing|20 |\n",
      "|Charlie|Sales    |30 |\n",
      "|Toto   |IT       |40 |\n",
      "+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Optimize the spark session config\n",
    "\n"
   ],
   "id": "5963e8a55526eefd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:26:44.047785Z",
     "start_time": "2025-08-12T13:26:39.081734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalMode_memo_config\")\n",
    "    .master(\"local[*]\")\n",
    "    # JVM memory allocation\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Half of RAM for driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "    # Shuffle & partition tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")  # Lower than default 200\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "    # Unified memory management\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")\n",
    "    # Spill to disk early instead of crashing\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    # optimize jvm GC\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    # Use Kryo serializer\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    # Optional: buffer size for serialization\n",
    "    .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "f220ba18441c3e6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> In this tutorial, we only focus on spark on local mode. CASD also proposes spark on `yarn` and `k8s` mode. For more information, please contact `service@casd.eu`.",
   "id": "27aa649ec04cfd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Read a csv file",
   "id": "6aa98d7f0a569594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:04:32.948774Z",
     "start_time": "2025-08-12T13:04:32.937204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root_dir = Path.cwd().parent\n",
    "data_dir = project_root_dir / \"data\"\n",
    "print(data_dir)"
   ],
   "id": "89718728313d2873",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLIU\\Documents\\git\\Seminar_PySpark_Sedona_GeoParquet\\data\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T12:47:51.834940Z",
     "start_time": "2025-08-12T12:47:51.534939Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_sample_file_path = data_dir / \"transactions_sample.csv\"\n",
    "\n",
    "# the option header\n",
    "sample_df = spark.read.csv(csv_sample_file_path.as_posix(), header=True, inferSchema=True)"
   ],
   "id": "1a381ae6fd78177",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T12:47:53.305411Z",
     "start_time": "2025-08-12T12:47:53.153582Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.show(5)",
   "id": "7151e23236639a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|id_transaction|date_transaction|    prix|departement|id_ville|               ville|code_postal|             adresse|type_batiment| vefa|n_pieces|surface_habitable|id_parcelle_cadastre|        latitude|       longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|      10160888|      2015-07-22|222500.0|         63|     247|               MUROL|      63790|         5148  COMBE|       Maison|False|       4|              123|      63247000ZO0165|45.5729726213494|2.94997597336221|                 {}|                        {}|                        {}|               {2387}|                     {}|\n",
      "|      10319766|      2024-06-18|218640.0|         73|      15|          LES ALLUES|      73550|45 RUE DU GRAND C...|  Appartement|False|       1|               23|      73015000AB0347|45.3987396582855|6.56760164004644|                {0}|                        {}|                        {}|                   {}|                     {}|\n",
      "|      11545562|      2020-07-23|254950.0|         77|     316|MORET-LOING-ET-OR...|      77250|1 RUE DE LA CROIX...|       Maison|False|       6|              124|      773161700B0305|48.3337616035885|2.78083033196279|                 {}|                        {}|                        {}|                {815}|                     {}|\n",
      "|      13891173|      2022-10-03|380000.0|         94|      81|     VITRY-SUR-SEINE|      94400|11 VOIE VICTOR MASSE|       Maison|False|       3|               93|      94081000AK0189| 48.796988276575|2.37555811653132|                 {}|                        {}|                        {}|                {208}|                     {}|\n",
      "|      11794772|      2018-07-12|258000.0|         77|     284|               MEAUX|      77100|14 RUE PIERRE BON...|       Maison|False|       5|               96|      77284000AW0090|48.9488159934431|2.89173438180231|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T12:47:54.411311Z",
     "start_time": "2025-08-12T12:47:54.403739Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.printSchema()",
   "id": "b764b67fad6f529c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: date (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: integer (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- vefa: string (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- id_parcelle_cadastre: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- surface_dependances: string (nullable = true)\n",
      " |-- surface_locaux_industriels: string (nullable = true)\n",
      " |-- surface_terrains_agricoles: string (nullable = true)\n",
      " |-- surface_terrains_sols: string (nullable = true)\n",
      " |-- surface_terrains_nature: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read a parquet file\n",
    "\n"
   ],
   "id": "15e77e2aea3f0aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:26:57.018055Z",
     "start_time": "2025-08-12T13:26:53.695037Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_transaction_path = \"C:/Users/PLIU/Documents/git/Seminar_PySpark_Sedona_GeoParquet/data/fr_immo_transaction.parquet\"\n",
    "fr_immo_transactions_df = spark.read.parquet(fr_immo_transaction_path)"
   ],
   "id": "446ea935a3539360",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:26:58.358996Z",
     "start_time": "2025-08-12T13:26:58.259235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "required_col = [\"id_transaction\", \"date_transaction\", \"prix\", \"departement\", \"ville\", \"code_postal\", \"adresse\",\n",
    "                \"type_batiment\", \"n_pieces\", \"surface_habitable\", \"latitude\", \"longitude\"]\n",
    "clean_fr_immo_df = fr_immo_transactions_df.select(required_col)"
   ],
   "id": "6fb940ee1aa45c3d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:03.761525Z",
     "start_time": "2025-08-12T13:27:00.940077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cache the dataframe for better performance\n",
    "# clean_fr_immo_df.cache()\n",
    "clean_fr_immo_df.show(5)"
   ],
   "id": "64bf43f61b938f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|               ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|        141653|      2014-01-02|197000.0|         01|             TREVOUX|       1600|  6346 MTE DES LILAS|  Appartement|       4|               84|45.9423014034837|4.77069364742062|\n",
      "|        141970|      2014-01-02|157500.0|         01|              VIRIAT|       1440|1369 RTE DE STRAS...|       Maison|       4|              103|46.2364072868351|5.26293493674271|\n",
      "|        139240|      2014-01-02|112000.0|         01|SAINT-JEAN-SUR-VEYLE|       1290|5174  SAINT JEAN ...|       Maison|       3|               78|46.2600767509964| 4.9186124567344|\n",
      "|        146016|      2014-01-02|173020.0|         01|             LAGNIEU|       1150|21 GR GRANDE RUE ...|       Maison|       4|               72|45.8990448750694|5.35421986570434|\n",
      "|        145911|      2014-01-03| 88000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       3|              104|46.2584083170745| 5.6408027525738|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean dataframe\n",
    "\n",
    "We want to check some basic information of the dataframe:\n",
    "- Total row count\n",
    "- schema(e.g.column name and data type)\n",
    "- Empty rows (all-null)\n",
    "- Rows with any missing values\n",
    "- Duplicate rows\n",
    "- Rows containing empty strings\n",
    "- Nulls count per column"
   ],
   "id": "b998c95a2e3087e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:07.032843Z",
     "start_time": "2025-08-12T13:27:07.014176Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import col, when, isnan, trim\n",
    "import pyspark.sql.types as spark_types\n",
    "\n",
    "\n",
    "def get_empty_row_count_per_column(df: DataFrame):\n",
    "    totalRowCount = df.count()\n",
    "\n",
    "    nullSymbols = [\"?\", \"-\"]\n",
    "    aggExpression = []\n",
    "\n",
    "    # step2: build the condition expression for detecting various null case\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        c = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # always test null\n",
    "        nullExpr = when(c.isNull(), 1).otherwise(0).alias(nullCountCol)\n",
    "        aggExpression.append(nullExpr)\n",
    "        # test isnan for only numeric columns\n",
    "        nanExpr = when(isnan(c), 1).otherwise(0).alias(nanCountCol)\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            aggExpression.append(nanExpr)\n",
    "        # string null value only for string columns\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            aggExpression.append(when(trim(c) == \"\", 1).otherwise(0).alias(blankCountCol))\n",
    "            aggExpression.append(when(c.isin(nullSymbols), 1).otherwise(0).alias(nullSymbolCountCol))\n",
    "\n",
    "    # Perform full-column conditional tagging\n",
    "    flaggedDf = df.select(*aggExpression)\n",
    "\n",
    "    # step3: sum all per-column null case flags in one single pass\n",
    "    try:\n",
    "        summed = flaggedDf.agg(*[spark_sum(c).alias(c) for c in flaggedDf.columns]).collect()[0].asDict()\n",
    "    except Exception as e:\n",
    "        print(f\"Aggregation failed on flaggedDf columns: {flaggedDf.columns}: {e}\")\n",
    "\n",
    "    result = []\n",
    "    # step4: build a list of dict which contains all info for the final result dataframe\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        nullCount = summed.get(nullCountCol, 0)\n",
    "        nanCount = summed.get(nanCountCol, 0)\n",
    "        blankCount = summed.get(blankCountCol, 0)\n",
    "        symbolCount = summed.get(nullSymbolCountCol, 0)\n",
    "        totalEmpty = nullCount + nanCount + blankCount + symbolCount\n",
    "\n",
    "        result.append((\n",
    "            colName, nullCount, nanCount, blankCount,\n",
    "            symbolCount, totalEmpty, totalRowCount\n",
    "        ))\n",
    "    # convert the list of dict into a new dataframe\n",
    "    resDf = spark.createDataFrame(result, [\"column_name\", \"null_count\", \"nan_count\", \"blank_count\",\n",
    "                                           \"null_symbol_count\", \"total_empty_row_count\",\n",
    "                                           \"total_row_count\"])\n",
    "    #\n",
    "\n",
    "    return resDf\n"
   ],
   "id": "649df8a50966851b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:08.205665Z",
     "start_time": "2025-08-12T13:27:08.194355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_duplicated_row_count(df: DataFrame):\n",
    "    duplicate_row_count = df.count() - df.dropDuplicates().count()\n",
    "    print(f\"Duplicate row count: {duplicate_row_count}\")\n"
   ],
   "id": "15c5dfa28326a370",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:13.313285Z",
     "start_time": "2025-08-12T13:27:08.907703Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats = get_empty_row_count_per_column(clean_fr_immo_df)",
   "id": "63785b8725cd4cba",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:20.737547Z",
     "start_time": "2025-08-12T13:27:13.345722Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats.show(20)",
   "id": "73face0dd0d004b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|      column_name|null_count|nan_count|blank_count|null_symbol_count|total_empty_row_count|total_row_count|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|   id_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "| date_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "|             prix|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      departement|         0|        0|          0|                0|                    0|        9141573|\n",
      "|            ville|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      code_postal|         0|        0|          0|                0|                    0|        9141573|\n",
      "|          adresse|         0|        0|          1|                0|                    1|        9141573|\n",
      "|    type_batiment|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         n_pieces|         0|        0|          0|                0|                    0|        9141573|\n",
      "|surface_habitable|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         latitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "|        longitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T13:27:51.598878Z",
     "start_time": "2025-08-12T13:27:51.367862Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sample_null_col_stats = get_empty_row_count_per_column(sample_df)\n",
    "sample_null_col_stats.show(20)"
   ],
   "id": "ef925c220db4f06d",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[10]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m sample_null_col_stats = get_empty_row_count_per_column(\u001B[43msample_df\u001B[49m)\n\u001B[32m      2\u001B[39m sample_null_col_stats.show(\u001B[32m20\u001B[39m)\n",
      "\u001B[31mNameError\u001B[39m: name 'sample_df' is not defined"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:42:44.549456Z",
     "start_time": "2025-08-06T13:42:26.397538Z"
    }
   },
   "cell_type": "code",
   "source": "get_duplicated_row_count(clean_fr_immo_df)",
   "id": "a4c6a7e9dc882ad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count: 0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T08:02:59.227161Z",
     "start_time": "2025-08-12T08:02:59.216977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "\n",
    "def has_value(df: DataFrame):\n",
    "    exprs = []\n",
    "    nullSymbols = [\"?\", \"-\"]\n",
    "    for colName in df.columns:\n",
    "        colRef = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # base condition, the given column is not null\n",
    "        conditions = [colRef.isNotNull()]\n",
    "\n",
    "        # for numeric column\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            conditions.append(~isnan(colRef))\n",
    "\n",
    "        # for string column\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            conditions.append(trim(colRef) != \"\")\n",
    "            conditions.append(~colRef.isin(nullSymbols))\n",
    "\n",
    "        # build final filter condition\n",
    "        hasValCond = conditions[0]\n",
    "        for cond in conditions[1:]:\n",
    "            hasValCond = hasValCond & cond\n",
    "        exprs.append(spark_max(when(hasValCond, 1).otherwise(0)).alias(colName))\n",
    "    result = df.agg(*exprs).collect()[0].asDict()\n",
    "    result[\"toto\"] = 0\n",
    "    return [c for c, has_value in result.items() if has_value == 0]\n",
    "    print(result)\n",
    "\n"
   ],
   "id": "2bdea439e9e09099",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T08:03:01.970332Z",
     "start_time": "2025-08-12T08:03:00.828547Z"
    }
   },
   "cell_type": "code",
   "source": "has_value(clean_fr_immo_df)",
   "id": "e9087e74d732c93c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toto']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# creating a geometry column\n",
    "\n"
   ],
   "id": "25189f710b996015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d30347ec5b67f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d427b8181b3b735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
