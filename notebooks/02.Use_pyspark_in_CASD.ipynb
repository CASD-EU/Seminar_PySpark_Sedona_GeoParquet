{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02.Use pyspark in CASD\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "- how to install spark and pyspark in CASD\n",
    "- how to create a spark session\n",
    "- read a csv file\n",
    "- read a parquet file\n",
    "\n",
    "\n",
    "## 1. Install spark and pyspark in CASD\n",
    "\n",
    "As we explained before,\n",
    "- `Apache Spark` is a distributed computation framework written mostly in Scala and Java, runs in a JVM(Java Virtual Machine)\n",
    "- `PySpark` is a Python API for Apache Spark.\n",
    "- PySpark talks to Spark engine via `Py4J`. Your Python code → Py4J(serialized and sent to JVM) → Spark core executes it → results sent back to python\n",
    "\n",
    "> Spark framework installation is essential, without it, pyspark will never work.\n",
    "\n",
    "### 1.1 Install spark framework\n",
    "\n",
    "CASD provides an installation script(`InstallSpark.ps1`) to install the `latest spark framework` and underlying JDK available in CASD.\n",
    "You can find this script in `Bureau->Raccourcis->Spark`.\n",
    "\n",
    "Open a powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# goto the target folder\n",
    "cd C:\\Users\\Public\\Desktop\\Raccourcis\\Spark\n",
    "\n",
    "# run the installation script\n",
    ".\\InstallSpark.ps1\n",
    "```\n",
    "\n",
    "> If everything works well, this script will install spark in `C:\\Users\\<your-id>\\AppData\\Local\\spark\\spark-3.5.5-bin-hadoop3`. It will also install open-jdk, winutils, and set up your\n",
    "> windows env vars.\n",
    "\n",
    "Now let's check if your spark works or not. Open a new powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# check the installed spark version\n",
    "spark-shell --version\n",
    "\n",
    "## it may take few seconds to show the output, be patient\n",
    "# expected output\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.2\n",
    "Branch HEAD\n",
    "Compiled by user ubuntu on 2024-08-06T11:36:15Z\n",
    "Revision bb7846dd487f259994fdc69e18e03382e3f64f42\n",
    "Url https://github.com/apache/spark\n",
    "Type --help for more information.\n",
    "```\n",
    "\n",
    "The\n",
    "> If you can't see the spark output, contact `service@casd.eu`\n",
    "\n",
    "### 1.2 Install pyspark\n",
    "\n",
    "As we mentioned before, CASD recommends you to create a `python virtual environment` for each for your python project.\n",
    "\n",
    "Suppose we will start a new project called `docs_in_paris`, let's create a python virtual environment with this name\n",
    "\n",
    "\n",
    "```powershell\n",
    "# create a python virtual environment\n",
    "conda create --name docs_in_paris python --offline\n",
    "\n",
    "# activate the virtual environment\n",
    "conda activate docs_in_paris\n",
    "\n",
    "# check python version\n",
    "python -V\n",
    "\n",
    "# check installed packages\n",
    "pip list\n",
    "\n",
    "# install pyspark\n",
    "pip install pyspark==3.5.5\n",
    "\n",
    "# check the installed pyspark version\n",
    "pip show pyspark\n",
    "\n",
    "# expected output\n",
    "Name: pyspark\n",
    "Version: 3.5.5\n",
    "Summary: Apache Spark Python API\n",
    "......\n",
    "```\n",
    "\n",
    "> You can notice that we have installed a specific version of pyspark. Because the pyspark version must be the same as the spark framework version. As the output in `section 1.1` is **spark-3.5.5**. So we need to\n",
    "> install pyspark-3.5.5\n",
    "\n",
    "## 2. Create a spark session\n",
    "\n",
    "A `Spark session` is the entry point to Apache Spark. A spark session allows us to interact with Spark’s core engine, no matter if you’re working in Python (PySpark), Scala, Java, or R.\n",
    "\n",
    "\n",
    "It encapsulates:\n",
    "\n",
    "- Cluster connection (or local JVM if local mode)\n",
    "- Configuration settings (memory, partitions, serializer, etc.)\n",
    "- Access to Spark’s APIs: Spark SQL API, DataFrame and Dataset API, RDD API (via .sparkContext), Streaming and machine learning APIs\n",
    "\n",
    "To create a spark session, you need to\n",
    "- import the required module\n",
    "- configure the spark session settings\n",
    "- create the spark session instance\n",
    "\n",
    "### 2.1 A minimum spark session creation\n",
    "\n",
    "Below shows a minimum spark session creation."
   ],
   "id": "b42cf971a64f18ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:28:29.899928Z",
     "start_time": "2025-08-12T07:28:28.386055Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession, DataFrame",
   "id": "6fbe633fa74cd51c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:29:02.773946Z",
     "start_time": "2025-08-12T07:28:29.911263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "    .appName(\"Use_pyspark_in_CASD\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "dbcc286d4b68fd6d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:29:03.437703Z",
     "start_time": "2025-08-12T07:29:03.397802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you can get and set configuration of your spark session any moments\n",
    "# get all conf\n",
    "spark.sparkContext.getConf().getAll()"
   ],
   "id": "45405f8a15ae8cea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.name', 'Use_pyspark_in_CASD'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.id', 'local-1754983741990'),\n",
       " ('spark.driver.host', 'LT-5CG4181HBL.casd.me'),\n",
       " ('spark.app.startTime', '1754983739966'),\n",
       " ('spark.driver.port', '51559'),\n",
       " ('spark.app.submitTime', '1754983739778'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:43:41.958729Z",
     "start_time": "2025-08-11T12:43:30.762311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType\n",
    "\n",
    "# create a dataframe by using List\n",
    "dept = [(\"Alice\",\"Finance\",10),\n",
    "        (\"Bob\",\"Marketing\",20),\n",
    "        (\"Charlie\",\"Sales\",30),\n",
    "        (\"Toto\",\"IT\",40)\n",
    "      ]\n",
    "\n",
    "# give an explicit schema\n",
    "deptSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ],
   "id": "f32a5bef0a3d4ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---------+---+\n",
      "|name   |dept_name|age|\n",
      "+-------+---------+---+\n",
      "|Alice  |Finance  |10 |\n",
      "|Bob    |Marketing|20 |\n",
      "|Charlie|Sales    |30 |\n",
      "|Toto   |IT       |40 |\n",
      "+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> In this tutorial, we only focus on spark on local mode. CASD also proposes spark on `yarn` and `k8s` mode. For more information, please contact `service@casd.eu`.",
   "id": "27aa649ec04cfd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Read a csv file",
   "id": "6aa98d7f0a569594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:21:53.419208Z",
     "start_time": "2025-08-11T13:21:48.889080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_sample_file_path = \"C:/Users/PLIU/Documents/ubuntu_share/data_set/france_immobilier/transactions_sample.csv\"\n",
    "\n",
    "# the option header\n",
    "sample_df = spark.read.csv(csv_sample_file_path, header=True, inferSchema=True)"
   ],
   "id": "1a381ae6fd78177",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:21:54.556687Z",
     "start_time": "2025-08-11T13:21:54.151986Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.show(5)",
   "id": "7151e23236639a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|id_transaction|date_transaction|    prix|departement|id_ville|               ville|code_postal|             adresse|type_batiment| vefa|n_pieces|surface_habitable|id_parcelle_cadastre|        latitude|       longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|      10160888|      2015-07-22|222500.0|         63|     247|               MUROL|      63790|         5148  COMBE|       Maison|false|       4|              123|      63247000ZO0165|45.5729726213494|2.94997597336221|                 {}|                        {}|                        {}|               {2387}|                     {}|\n",
      "|      10319766|      2024-06-18|218640.0|         73|      15|          LES ALLUES|      73550|45 RUE DU GRAND C...|  Appartement|false|       1|               23|      73015000AB0347|45.3987396582855|6.56760164004644|                {0}|                        {}|                        {}|                   {}|                     {}|\n",
      "|      11545562|      2020-07-23|254950.0|         77|     316|MORET-LOING-ET-OR...|      77250|1 RUE DE LA CROIX...|       Maison|false|       6|              124|      773161700B0305|48.3337616035885|2.78083033196279|                 {}|                        {}|                        {}|                {815}|                     {}|\n",
      "|      13891173|      2022-10-03|380000.0|         94|      81|     VITRY-SUR-SEINE|      94400|11 VOIE VICTOR MASSE|       Maison|false|       3|               93|      94081000AK0189| 48.796988276575|2.37555811653132|                 {}|                        {}|                        {}|                {208}|                     {}|\n",
      "|      11794772|      2018-07-12|258000.0|         77|     284|               MEAUX|      77100|14 RUE PIERRE BON...|       Maison|false|       5|               96|      77284000AW0090|48.9488159934431|2.89173438180231|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:21:57.515140Z",
     "start_time": "2025-08-11T13:21:57.507363Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.printSchema()",
   "id": "b764b67fad6f529c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: date (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: integer (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- vefa: boolean (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- id_parcelle_cadastre: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- surface_dependances: string (nullable = true)\n",
      " |-- surface_locaux_industriels: string (nullable = true)\n",
      " |-- surface_terrains_agricoles: string (nullable = true)\n",
      " |-- surface_terrains_sols: string (nullable = true)\n",
      " |-- surface_terrains_nature: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read a parquet file\n",
    "\n"
   ],
   "id": "15e77e2aea3f0aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:29:07.697607Z",
     "start_time": "2025-08-12T07:29:03.528961Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_transaction_path = \"C:/Users/PLIU/Documents/git/Seminar_PySpark_Sedona_GeoParquet/data/fr_immo_transaction.parquet\"\n",
    "fr_immo_transactions_df = spark.read.parquet(fr_immo_transaction_path)"
   ],
   "id": "446ea935a3539360",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:29:07.835002Z",
     "start_time": "2025-08-12T07:29:07.756762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "required_col = [\"id_transaction\",\"date_transaction\",\"prix\",\"departement\",\"ville\",\"code_postal\",\"adresse\",\"type_batiment\",\"n_pieces\",\"surface_habitable\",\"latitude\",\"longitude\"]\n",
    "clean_fr_immo_df = fr_immo_transactions_df.select(required_col)"
   ],
   "id": "6fb940ee1aa45c3d",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:29:10.843785Z",
     "start_time": "2025-08-12T07:29:07.841352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cache the dataframe for better performence\n",
    "# clean_fr_immo_df.cache()\n",
    "clean_fr_immo_df.show(5)"
   ],
   "id": "64bf43f61b938f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|               ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|        141653|      2014-01-02|197000.0|         01|             TREVOUX|       1600|  6346 MTE DES LILAS|  Appartement|       4|               84|45.9423014034837|4.77069364742062|\n",
      "|        141970|      2014-01-02|157500.0|         01|              VIRIAT|       1440|1369 RTE DE STRAS...|       Maison|       4|              103|46.2364072868351|5.26293493674271|\n",
      "|        139240|      2014-01-02|112000.0|         01|SAINT-JEAN-SUR-VEYLE|       1290|5174  SAINT JEAN ...|       Maison|       3|               78|46.2600767509964| 4.9186124567344|\n",
      "|        146016|      2014-01-02|173020.0|         01|             LAGNIEU|       1150|21 GR GRANDE RUE ...|       Maison|       4|               72|45.8990448750694|5.35421986570434|\n",
      "|        145911|      2014-01-03| 88000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       3|              104|46.2584083170745| 5.6408027525738|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean dataframe\n",
    "\n",
    "We want to check some basic information of the dataframe:\n",
    "- Total row count\n",
    "- schema(e.g.column name and data type)\n",
    "- Empty rows (all-null)\n",
    "- Rows with any missing values\n",
    "- Duplicate rows\n",
    "- Rows containing empty strings\n",
    "- Nulls count per column"
   ],
   "id": "b998c95a2e3087e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:33:06.552455Z",
     "start_time": "2025-08-12T07:33:06.539681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import col, when, isnan, trim\n",
    "import pyspark.sql.types as spark_types\n",
    "\n",
    "def get_empty_row_count_per_column(df:DataFrame):\n",
    "    totalRowCount = df.count()\n",
    "\n",
    "    nullSymbols = [\"?\",\"-\"]\n",
    "    aggExpression = []\n",
    "\n",
    "    # step2: build the condition expression for detecting various null case\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        c = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # always test null\n",
    "        nullExpr = when(c.isNull(), 1).otherwise(0).alias(nullCountCol)\n",
    "        aggExpression.append(nullExpr)\n",
    "        # test isnan for only numeric columns\n",
    "        nanExpr = when(isnan(c), 1).otherwise(0).alias(nanCountCol)\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            aggExpression.append(nanExpr)\n",
    "        # string null value only for string columns\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            aggExpression.append(when(trim(c) == \"\", 1).otherwise(0).alias(blankCountCol))\n",
    "            aggExpression.append(when(c.isin(nullSymbols), 1).otherwise(0).alias(nullSymbolCountCol))\n",
    "\n",
    "    # show the agg expression\n",
    "    for aggExpr in aggExpression:\n",
    "        print(aggExpr)\n",
    "    # Perform full-column conditional tagging\n",
    "    flaggedDf = df.select(*aggExpression)\n",
    "    flaggedDf.show(5)\n",
    "\n",
    "    # step3: sum all per-column null case flags in one single pass\n",
    "    try:\n",
    "        summed = flaggedDf.agg(*[spark_sum(c).alias(c) for c in flaggedDf.columns]).collect()[0].asDict()\n",
    "    except Exception as e:\n",
    "        print(f\"Aggregation failed on flaggedDf columns: {flaggedDf.columns}: {e}\")\n",
    "\n",
    "    result = []\n",
    "    # step4: build a list of dict which contains all info for the final result dataframe\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        nullCount = summed.get(nullCountCol, 0)\n",
    "        nanCount = summed.get(nanCountCol, 0)\n",
    "        blankCount = summed.get(blankCountCol, 0)\n",
    "        symbolCount = summed.get(nullSymbolCountCol, 0)\n",
    "        totalEmpty = nullCount + nanCount + blankCount + symbolCount\n",
    "\n",
    "        result.append((\n",
    "            colName, nullCount, nanCount, blankCount,\n",
    "            symbolCount, totalEmpty, totalRowCount\n",
    "        ))\n",
    "    # convert the list of dict into a new dataframe\n",
    "    resDf = spark.createDataFrame(result, [\"column_name\", \"null_count\", \"nan_count\", \"blank_count\",\n",
    "                                                \"null_symbol_count\", \"total_empty_row_count\",\n",
    "                                                \"total_row_count\"])\n",
    "    #\n",
    "\n",
    "    return resDf\n"
   ],
   "id": "649df8a50966851b",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T13:23:42.419643Z",
     "start_time": "2025-08-11T13:23:42.409775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_duplicated_row_count(df:DataFrame):\n",
    "     duplicate_row_count = df.count() - df.dropDuplicates().count()\n",
    "     print(f\"Duplicate row count: {duplicate_row_count}\")\n"
   ],
   "id": "15c5dfa28326a370",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T07:33:11.996551Z",
     "start_time": "2025-08-12T07:33:09.912232Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats = get_empty_row_count_per_column(clean_fr_immo_df)",
   "id": "63785b8725cd4cba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'CASE WHEN (id_transaction IS NULL) THEN 1 ELSE 0 END AS id_transaction__null'>\n",
      "Column<'CASE WHEN isnan(id_transaction) THEN 1 ELSE 0 END AS id_transaction__nan'>\n",
      "Column<'CASE WHEN (date_transaction IS NULL) THEN 1 ELSE 0 END AS date_transaction__null'>\n",
      "Column<'CASE WHEN (prix IS NULL) THEN 1 ELSE 0 END AS prix__null'>\n",
      "Column<'CASE WHEN isnan(prix) THEN 1 ELSE 0 END AS prix__nan'>\n",
      "Column<'CASE WHEN (departement IS NULL) THEN 1 ELSE 0 END AS departement__null'>\n",
      "Column<'CASE WHEN (trim(departement) = ) THEN 1 ELSE 0 END AS departement__blank'>\n",
      "Column<'CASE WHEN (departement IN (?, -)) THEN 1 ELSE 0 END AS departement__symbol'>\n",
      "Column<'CASE WHEN (ville IS NULL) THEN 1 ELSE 0 END AS ville__null'>\n",
      "Column<'CASE WHEN (trim(ville) = ) THEN 1 ELSE 0 END AS ville__blank'>\n",
      "Column<'CASE WHEN (ville IN (?, -)) THEN 1 ELSE 0 END AS ville__symbol'>\n",
      "Column<'CASE WHEN (code_postal IS NULL) THEN 1 ELSE 0 END AS code_postal__null'>\n",
      "Column<'CASE WHEN isnan(code_postal) THEN 1 ELSE 0 END AS code_postal__nan'>\n",
      "Column<'CASE WHEN (adresse IS NULL) THEN 1 ELSE 0 END AS adresse__null'>\n",
      "Column<'CASE WHEN (trim(adresse) = ) THEN 1 ELSE 0 END AS adresse__blank'>\n",
      "Column<'CASE WHEN (adresse IN (?, -)) THEN 1 ELSE 0 END AS adresse__symbol'>\n",
      "Column<'CASE WHEN (type_batiment IS NULL) THEN 1 ELSE 0 END AS type_batiment__null'>\n",
      "Column<'CASE WHEN (trim(type_batiment) = ) THEN 1 ELSE 0 END AS type_batiment__blank'>\n",
      "Column<'CASE WHEN (type_batiment IN (?, -)) THEN 1 ELSE 0 END AS type_batiment__symbol'>\n",
      "Column<'CASE WHEN (n_pieces IS NULL) THEN 1 ELSE 0 END AS n_pieces__null'>\n",
      "Column<'CASE WHEN isnan(n_pieces) THEN 1 ELSE 0 END AS n_pieces__nan'>\n",
      "Column<'CASE WHEN (surface_habitable IS NULL) THEN 1 ELSE 0 END AS surface_habitable__null'>\n",
      "Column<'CASE WHEN isnan(surface_habitable) THEN 1 ELSE 0 END AS surface_habitable__nan'>\n",
      "Column<'CASE WHEN (latitude IS NULL) THEN 1 ELSE 0 END AS latitude__null'>\n",
      "Column<'CASE WHEN isnan(latitude) THEN 1 ELSE 0 END AS latitude__nan'>\n",
      "Column<'CASE WHEN (longitude IS NULL) THEN 1 ELSE 0 END AS longitude__null'>\n",
      "Column<'CASE WHEN isnan(longitude) THEN 1 ELSE 0 END AS longitude__nan'>\n",
      "+--------------------+-------------------+----------------------+----------+---------+-----------------+------------------+-------------------+-----------+------------+-------------+-----------------+----------------+-------------+--------------+---------------+-------------------+--------------------+---------------------+--------------+-------------+-----------------------+----------------------+--------------+-------------+---------------+--------------+\n",
      "|id_transaction__null|id_transaction__nan|date_transaction__null|prix__null|prix__nan|departement__null|departement__blank|departement__symbol|ville__null|ville__blank|ville__symbol|code_postal__null|code_postal__nan|adresse__null|adresse__blank|adresse__symbol|type_batiment__null|type_batiment__blank|type_batiment__symbol|n_pieces__null|n_pieces__nan|surface_habitable__null|surface_habitable__nan|latitude__null|latitude__nan|longitude__null|longitude__nan|\n",
      "+--------------------+-------------------+----------------------+----------+---------+-----------------+------------------+-------------------+-----------+------------+-------------+-----------------+----------------+-------------+--------------+---------------+-------------------+--------------------+---------------------+--------------+-------------+-----------------------+----------------------+--------------+-------------+---------------+--------------+\n",
      "|                   0|                  0|                     0|         0|        0|                0|                 0|                  0|          0|           0|            0|                0|               0|            0|             0|              0|                  0|                   0|                    0|             0|            0|                      0|                     0|             0|            0|              0|             0|\n",
      "|                   0|                  0|                     0|         0|        0|                0|                 0|                  0|          0|           0|            0|                0|               0|            0|             0|              0|                  0|                   0|                    0|             0|            0|                      0|                     0|             0|            0|              0|             0|\n",
      "|                   0|                  0|                     0|         0|        0|                0|                 0|                  0|          0|           0|            0|                0|               0|            0|             0|              0|                  0|                   0|                    0|             0|            0|                      0|                     0|             0|            0|              0|             0|\n",
      "|                   0|                  0|                     0|         0|        0|                0|                 0|                  0|          0|           0|            0|                0|               0|            0|             0|              0|                  0|                   0|                    0|             0|            0|                      0|                     0|             0|            0|              0|             0|\n",
      "|                   0|                  0|                     0|         0|        0|                0|                 0|                  0|          0|           0|            0|                0|               0|            0|             0|              0|                  0|                   0|                    0|             0|            0|                      0|                     0|             0|            0|              0|             0|\n",
      "+--------------------+-------------------+----------------------+----------+---------+-----------------+------------------+-------------------+-----------+------------+-------------+-----------------+----------------+-------------+--------------+---------------+-------------------+--------------------+---------------------+--------------+-------------+-----------------------+----------------------+--------------+-------------+---------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T08:03:44.651744Z",
     "start_time": "2025-08-12T08:03:37.468102Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats.show(20)",
   "id": "73face0dd0d004b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|      column_name|null_count|nan_count|blank_count|null_symbol_count|total_empty_row_count|total_row_count|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|   id_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "| date_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "|             prix|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      departement|         0|        0|          0|                0|                    0|        9141573|\n",
      "|            ville|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      code_postal|         0|        0|          0|                0|                    0|        9141573|\n",
      "|          adresse|         0|        0|          1|                0|                    1|        9141573|\n",
      "|    type_batiment|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         n_pieces|         0|        0|          0|                0|                    0|        9141573|\n",
      "|surface_habitable|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         latitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "|        longitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:42:44.549456Z",
     "start_time": "2025-08-06T13:42:26.397538Z"
    }
   },
   "cell_type": "code",
   "source": "get_duplicated_row_count(clean_fr_immo_df)",
   "id": "a4c6a7e9dc882ad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count: 0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T08:02:59.227161Z",
     "start_time": "2025-08-12T08:02:59.216977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import max as spark_max\n",
    "\n",
    "def has_value(df:DataFrame):\n",
    "    exprs = []\n",
    "    nullSymbols = [\"?\",\"-\"]\n",
    "    for colName in df.columns:\n",
    "        colRef = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # base condition, the given column is not null\n",
    "        conditions = [colRef.isNotNull()]\n",
    "\n",
    "        # for numeric column\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            conditions.append(~isnan(colRef))\n",
    "\n",
    "        # for string column\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            conditions.append(trim(colRef) != \"\")\n",
    "            conditions.append(~colRef.isin(nullSymbols))\n",
    "\n",
    "        # build final filter condition\n",
    "        hasValCond = conditions[0]\n",
    "        for cond in conditions[1:]:\n",
    "            hasValCond = hasValCond & cond\n",
    "        exprs.append(spark_max(when(hasValCond,1).otherwise(0)).alias(colName))\n",
    "    result = df.agg(*exprs).collect()[0].asDict()\n",
    "    result[\"toto\"] = 0\n",
    "    return [c for c, has_value in result.items() if has_value == 0]\n",
    "    print(result)\n",
    "\n"
   ],
   "id": "2bdea439e9e09099",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T08:03:01.970332Z",
     "start_time": "2025-08-12T08:03:00.828547Z"
    }
   },
   "cell_type": "code",
   "source": "has_value(clean_fr_immo_df)",
   "id": "e9087e74d732c93c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toto']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# creating a geometry column\n",
    "\n"
   ],
   "id": "25189f710b996015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d30347ec5b67f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d427b8181b3b735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
