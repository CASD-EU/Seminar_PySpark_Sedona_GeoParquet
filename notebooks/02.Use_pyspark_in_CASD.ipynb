{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02.Use pyspark in CASD\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "- how to install spark and pyspark in CASD\n",
    "- how to create a spark session\n",
    "- read a csv file\n",
    "- read a parquet file\n",
    "\n",
    "\n",
    "## 1. Install spark and pyspark in CASD\n",
    "\n",
    "As we explained before,\n",
    "- `Apache Spark` is a distributed computation framework written mostly in Scala and Java, runs in a JVM(Java Virtual Machine)\n",
    "- `PySpark` is a Python API for Apache Spark.\n",
    "- PySpark talks to Spark engine via `Py4J`. Your Python code → Py4J(serialized and sent to JVM) → Spark core executes it → results sent back to python\n",
    "\n",
    "> Spark framework installation is essential, without it, pyspark will never work.\n",
    "\n",
    "### 1.1 Install spark framework\n",
    "\n",
    "CASD provides an installation script(`InstallSpark.ps1`) to install the `latest spark framework` and underlying JDK available in CASD.\n",
    "You can find this script in `Bureau->Raccourcis->Spark`.\n",
    "\n",
    "Open a powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# goto the target folder\n",
    "cd C:\\Users\\Public\\Desktop\\Raccourcis\\Spark\n",
    "\n",
    "# run the installation script\n",
    ".\\InstallSpark.ps1\n",
    "```\n",
    "\n",
    "> If everything works well, this script will install spark in `C:\\Users\\<your-id>\\AppData\\Local\\spark\\spark-3.5.5-bin-hadoop3`. It will also install open-jdk, winutils, and set up your\n",
    "> windows env vars.\n",
    "\n",
    "Now let's check if your spark works or not. Open a new powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# check the installed spark version\n",
    "spark-shell --version\n",
    "\n",
    "## it may take few seconds to show the output, be patient\n",
    "# expected output\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.2\n",
    "Branch HEAD\n",
    "Compiled by user ubuntu on 2024-08-06T11:36:15Z\n",
    "Revision bb7846dd487f259994fdc69e18e03382e3f64f42\n",
    "Url https://github.com/apache/spark\n",
    "Type --help for more information.\n",
    "```\n",
    "\n",
    "The\n",
    "> If you can't see the spark output, contact `service@casd.eu`\n",
    "\n",
    "### 1.2 Install pyspark\n",
    "\n",
    "As we mentioned before, CASD recommends you to create a `python virtual environment` for each for your python project.\n",
    "\n",
    "Suppose we will start a new project called `docs_in_paris`, let's create a python virtual environment with this name\n",
    "\n",
    "\n",
    "```powershell\n",
    "# create a python virtual environment\n",
    "conda create --name docs_in_paris python --offline\n",
    "\n",
    "# activate the virtual environment\n",
    "conda activate docs_in_paris\n",
    "\n",
    "# check python version\n",
    "python -V\n",
    "\n",
    "# check installed packages\n",
    "pip list\n",
    "\n",
    "# install pyspark\n",
    "pip install pyspark==3.5.5\n",
    "\n",
    "# check the installed pyspark version\n",
    "pip show pyspark\n",
    "\n",
    "# expected output\n",
    "Name: pyspark\n",
    "Version: 3.5.5\n",
    "Summary: Apache Spark Python API\n",
    "......\n",
    "```\n",
    "\n",
    "> You can notice that we have installed a specific version of pyspark. Because the pyspark version must be the same as the spark framework version. As the output in `section 1.1` is **spark-3.5.5**. So we need to\n",
    "> install pyspark-3.5.5\n",
    "\n",
    "## 2. Create a spark session\n",
    "\n",
    "A `Spark session` is the entry point to Apache Spark. A spark session allows us to interact with Spark’s core engine, no matter if you’re working in Python (PySpark), Scala, Java, or R.\n",
    "\n",
    "\n",
    "It encapsulates:\n",
    "\n",
    "- Cluster connection (or local JVM if local mode)\n",
    "- Configuration settings (memory, partitions, serializer, etc.)\n",
    "- Access to Spark’s APIs: Spark SQL API, DataFrame and Dataset API, RDD API (via .sparkContext), Streaming and machine learning APIs\n",
    "\n",
    "To create a spark session, you need to\n",
    "- import the required module\n",
    "- configure the spark session settings\n",
    "- create the spark session instance\n",
    "\n",
    "### 2.1 A minimum spark session creation\n",
    "\n",
    "Below shows a minimum spark session creation."
   ],
   "id": "b42cf971a64f18ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:42:06.735484Z",
     "start_time": "2025-08-11T12:42:06.566053Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession, DataFrame",
   "id": "6fbe633fa74cd51c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:42:44.318612Z",
     "start_time": "2025-08-11T12:42:06.746807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "     .master(\"local[*]\") \\\n",
    "    .appName(\"Use_pyspark_in_CASD\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "dbcc286d4b68fd6d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:42:45.436286Z",
     "start_time": "2025-08-11T12:42:45.333915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you can get and set configuration of your spark session any moments\n",
    "# get all conf\n",
    "spark.sparkContext.getConf().getAll()"
   ],
   "id": "45405f8a15ae8cea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.name', 'Use_pyspark_in_CASD'),\n",
       " ('spark.app.startTime', '1754916159775'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'LT-5CG4181HBL.casd.me'),\n",
       " ('spark.driver.port', '51146'),\n",
       " ('spark.app.id', 'local-1754916163135'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.app.submitTime', '1754916159591'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-11T12:43:41.958729Z",
     "start_time": "2025-08-11T12:43:30.762311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType\n",
    "\n",
    "# create a dataframe by using List\n",
    "dept = [(\"Alice\",\"Finance\",10),\n",
    "        (\"Bob\",\"Marketing\",20),\n",
    "        (\"Charlie\",\"Sales\",30),\n",
    "        (\"Toto\",\"IT\",40)\n",
    "      ]\n",
    "\n",
    "# give an explicit schema\n",
    "deptSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "\n",
    "# create dataframe\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema = deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ],
   "id": "f32a5bef0a3d4ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---------+---+\n",
      "|name   |dept_name|age|\n",
      "+-------+---------+---+\n",
      "|Alice  |Finance  |10 |\n",
      "|Bob    |Marketing|20 |\n",
      "|Charlie|Sales    |30 |\n",
      "|Toto   |IT       |40 |\n",
      "+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> In this tutorial, we only focus on spark on local mode. CASD also proposes spark on `yarn` and `k8s` mode. For more information, please contact `service@casd.eu`.",
   "id": "27aa649ec04cfd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Read a csv file",
   "id": "6aa98d7f0a569594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:31:38.345641Z",
     "start_time": "2025-08-06T13:31:33.364489Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_sample_file_path = \"C:/Users/PLIU/Documents/ubuntu_share/data_set/france_immobilier/transactions_sample.csv\"\n",
    "\n",
    "# the option header\n",
    "sample_df = spark.read.csv(csv_sample_file_path, header=True, inferSchema=True)"
   ],
   "id": "1a381ae6fd78177",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:31:42.602101Z",
     "start_time": "2025-08-06T13:31:42.249924Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.show(5)",
   "id": "7151e23236639a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|id_transaction|date_transaction|    prix|departement|id_ville|               ville|code_postal|             adresse|type_batiment| vefa|n_pieces|surface_habitable|id_parcelle_cadastre|        latitude|       longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|      10160888|      2015-07-22|222500.0|         63|     247|               MUROL|      63790|         5148  COMBE|       Maison|false|       4|              123|      63247000ZO0165|45.5729726213494|2.94997597336221|                 {}|                        {}|                        {}|               {2387}|                     {}|\n",
      "|      10319766|      2024-06-18|218640.0|         73|      15|          LES ALLUES|      73550|45 RUE DU GRAND C...|  Appartement|false|       1|               23|      73015000AB0347|45.3987396582855|6.56760164004644|                {0}|                        {}|                        {}|                   {}|                     {}|\n",
      "|      11545562|      2020-07-23|254950.0|         77|     316|MORET-LOING-ET-OR...|      77250|1 RUE DE LA CROIX...|       Maison|false|       6|              124|      773161700B0305|48.3337616035885|2.78083033196279|                 {}|                        {}|                        {}|                {815}|                     {}|\n",
      "|      13891173|      2022-10-03|380000.0|         94|      81|     VITRY-SUR-SEINE|      94400|11 VOIE VICTOR MASSE|       Maison|false|       3|               93|      94081000AK0189| 48.796988276575|2.37555811653132|                 {}|                        {}|                        {}|                {208}|                     {}|\n",
      "|      11794772|      2018-07-12|258000.0|         77|     284|               MEAUX|      77100|14 RUE PIERRE BON...|       Maison|false|       5|               96|      77284000AW0090|48.9488159934431|2.89173438180231|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:33:00.299208Z",
     "start_time": "2025-08-06T13:33:00.292776Z"
    }
   },
   "cell_type": "code",
   "source": "sample_df.printSchema()",
   "id": "b764b67fad6f529c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: date (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: integer (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- vefa: boolean (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- id_parcelle_cadastre: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- surface_dependances: string (nullable = true)\n",
      " |-- surface_locaux_industriels: string (nullable = true)\n",
      " |-- surface_terrains_agricoles: string (nullable = true)\n",
      " |-- surface_terrains_sols: string (nullable = true)\n",
      " |-- surface_terrains_nature: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read a parquet file\n",
    "\n"
   ],
   "id": "15e77e2aea3f0aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:33:07.662096Z",
     "start_time": "2025-08-06T13:33:07.231341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_transaction_path = \"C:/Users/PLIU/Documents/git/Seminar_PySpark_Sedona_GeoParquet/data/fr_immo_transaction.parquet\"\n",
    "fr_immo_transactions_df = spark.read.parquet(fr_immo_transaction_path)"
   ],
   "id": "446ea935a3539360",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:33:08.225438Z",
     "start_time": "2025-08-06T13:33:08.167123Z"
    }
   },
   "cell_type": "code",
   "source": [
    "required_col = [\"id_transaction\",\"date_transaction\",\"prix\",\"departement\",\"ville\",\"code_postal\",\"adresse\",\"type_batiment\",\"n_pieces\",\"surface_habitable\",\"latitude\",\"longitude\"]\n",
    "clean_fr_immo_df = fr_immo_transactions_df.select(required_col)"
   ],
   "id": "6fb940ee1aa45c3d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:41:26.611783Z",
     "start_time": "2025-08-06T13:41:26.507557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cache the dataframe for better performence\n",
    "clean_fr_immo_df.cache()\n",
    "clean_fr_immo_df.show()"
   ],
   "id": "64bf43f61b938f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|               ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|        141653|      2014-01-02|197000.0|         01|             TREVOUX|       1600|  6346 MTE DES LILAS|  Appartement|       4|               84|45.9423014034837|4.77069364742062|\n",
      "|        141970|      2014-01-02|157500.0|         01|              VIRIAT|       1440|1369 RTE DE STRAS...|       Maison|       4|              103|46.2364072868351|5.26293493674271|\n",
      "|        139240|      2014-01-02|112000.0|         01|SAINT-JEAN-SUR-VEYLE|       1290|5174  SAINT JEAN ...|       Maison|       3|               78|46.2600767509964| 4.9186124567344|\n",
      "|        146016|      2014-01-02|173020.0|         01|             LAGNIEU|       1150|21 GR GRANDE RUE ...|       Maison|       4|               72|45.8990448750694|5.35421986570434|\n",
      "|        145911|      2014-01-03| 88000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       3|              104|46.2584083170745| 5.6408027525738|\n",
      "|        145024|      2014-01-03|202500.0|         01|ST ETIENNE-SUR-RE...|       1190|    5112  AU MOIROUX|       Maison|       6|              118|46.3900603987491|5.01770950682101|\n",
      "|        148141|      2014-01-03| 49023.3|         01|               BALAN|       1360|7 LOT AFU DE BARB...|       Maison|       5|              105|45.8321367961343|5.09791701908164|\n",
      "|        145399|      2014-01-03| 68000.0|         01|     BOURG-EN-BRESSE|       1000|10Z RUE EDGAR  QU...|  Appartement|       2|               44|  46.20385904116|5.22569295332627|\n",
      "|        143192|      2014-01-03|156750.0|         01|     BOURG-EN-BRESSE|       1000| 15 RUE DE MONTHOLON|  Appartement|       3|               83|46.2000838066075|5.21083723376748|\n",
      "|        146426|      2014-01-03|163640.0|         01|SAINT-MAURICE-DE-...|       1700|      40 AV DES ILES|  Appartement|       3|               84|45.8232892882996|4.97719901458052|\n",
      "|        141750|      2014-01-03|203000.0|         01|             DAGNEUX|       1120|733C RTE DE STE C...|       Maison|       5|               90|45.8599182503004|5.05532938199608|\n",
      "|        148109|      2014-01-03|126000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       4|              111|46.2584083170745| 5.6408027525738|\n",
      "|        146587|      2014-01-03|145000.0|         01|            FARAMANS|       1800|100 CHE DE LA COT...|       Maison|       4|               88|45.8984143194015| 5.1238429973443|\n",
      "|        142324|      2014-01-06|180230.0|         01|SAINT-GENIS-SUR-M...|       1380|   5013  LA TERRASSE|       Maison|       5|              200|46.3005918214866|5.01292534875678|\n",
      "|        147770|      2014-01-06|107000.0|         01|     BOURG-EN-BRESSE|       1000|18 RUE GEORGES GU...|  Appartement|       4|               81|46.1940166503064| 5.2085969704023|\n",
      "|        140120|      2014-01-06|177500.0|         01|BELLEGARDE-SUR-VA...|       1200|    23 RUE DE MUSSEL|       Maison|       6|              120|46.1012359876184|5.81606709511131|\n",
      "|        142088|      2014-01-06|165000.0|         01|BELLEGARDE-SUR-VA...|       1200|        10 RUE VIALA|  Appartement|       4|               87|46.1065010130505|5.82630764368207|\n",
      "|        146625|      2014-01-06|152000.0|         01|           BELLIGNAT|       1100|16 RUE GUSTAVE FL...|       Maison|       5|               92|46.2476079780747|5.63367658432816|\n",
      "|        146683|      2014-01-06|415000.0|         01|                 GEX|       1170|    111 RUE DE PARIS|  Appartement|       5|              117|46.3369095806716|6.05697509698794|\n",
      "|        144187|      2014-01-06|430500.0|         01|           MONTHIEUX|       1390|   300 GR GRANDE RUE|       Maison|       5|              175| 45.954218351011|4.94060008338832|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Clean dataframe\n",
    "\n",
    "We want to check some basic information of the dataframe:\n",
    "- Total row count\n",
    "- schema(e.g.column name and data type)\n",
    "- Empty rows (all-null)\n",
    "- Rows with any missing values\n",
    "- Duplicate rows\n",
    "- Rows containing empty strings\n",
    "- Nulls count per column"
   ],
   "id": "b998c95a2e3087e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:31:49.406227Z",
     "start_time": "2025-08-06T13:31:49.381229Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from pyspark.sql.functions import col, when, isnan, trim\n",
    "import pyspark.sql.types as spark_types\n",
    "\n",
    "def get_empty_row_count_per_column(df:DataFrame):\n",
    "    totalRowCount = df.count()\n",
    "\n",
    "    nullSymbols = [\"?\",\"-\"]\n",
    "    aggExpression = []\n",
    "\n",
    "    # step2: build the condition expression for detecting various null case\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        c = col(colName)\n",
    "        colType = df.schema[colName].dataType\n",
    "        # always test null\n",
    "        nullExpr = when(c.isNull(), 1).otherwise(0).alias(nullCountCol)\n",
    "        aggExpression.append(nullExpr)\n",
    "        # test isnan for only numeric columns\n",
    "        nanExpr = when(isnan(c), 1).otherwise(0).alias(nanCountCol)\n",
    "        if isinstance(colType, spark_types.NumericType):\n",
    "            aggExpression.append(nanExpr)\n",
    "        # string null value only for string columns\n",
    "        if isinstance(colType, spark_types.StringType):\n",
    "            aggExpression.append(when(trim(c) == \"\", 1).otherwise(0).alias(blankCountCol))\n",
    "            aggExpression.append(when(c.isin(nullSymbols), 1).otherwise(0).alias(nullSymbolCountCol))\n",
    "\n",
    "        # Perform full-column conditional tagging\n",
    "    flaggedDf = df.select(*aggExpression)\n",
    "\n",
    "    # step3: sum all per-column null case flags in one single pass\n",
    "    try:\n",
    "        summed = flaggedDf.agg(*[spark_sum(c).alias(c) for c in flaggedDf.columns]).collect()[0].asDict()\n",
    "    except Exception as e:\n",
    "        print(f\"Aggregation failed on flaggedDf columns: {flaggedDf.columns}: {e}\")\n",
    "\n",
    "    result = []\n",
    "    # step4: build a list of dict which contains all info for the final result dataframe\n",
    "    for colName in df.columns:\n",
    "        # temporal col name\n",
    "        nullCountCol = f\"{colName}__null\"\n",
    "        nanCountCol = f\"{colName}__nan\"\n",
    "        blankCountCol = f\"{colName}__blank\"\n",
    "        nullSymbolCountCol = f\"{colName}__symbol\"\n",
    "        nullCount = summed.get(nullCountCol, 0)\n",
    "        nanCount = summed.get(nanCountCol, 0)\n",
    "        blankCount = summed.get(blankCountCol, 0)\n",
    "        symbolCount = summed.get(nullSymbolCountCol, 0)\n",
    "        totalEmpty = nullCount + nanCount + blankCount + symbolCount\n",
    "\n",
    "        result.append((\n",
    "            colName, nullCount, nanCount, blankCount,\n",
    "            symbolCount, totalEmpty, totalRowCount\n",
    "        ))\n",
    "    # convert the list of dict into a new dataframe\n",
    "    resDf = spark.createDataFrame(result, [\"column_name\", \"null_count\", \"nan_count\", \"blank_count\",\n",
    "                                                \"null_symbol_count\", \"total_empty_row_count\",\n",
    "                                                \"total_row_count\"])\n",
    "    #\n",
    "\n",
    "    return resDf\n"
   ],
   "id": "649df8a50966851b",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:42:16.048356Z",
     "start_time": "2025-08-06T13:42:16.042357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_duplicated_row_count(df:DataFrame):\n",
    "     duplicate_row_count = df.count() - df.dropDuplicates().count()\n",
    "     print(f\"Duplicate row count: {duplicate_row_count}\")\n"
   ],
   "id": "15c5dfa28326a370",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:33:46.093926Z",
     "start_time": "2025-08-06T13:33:37.642930Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats = get_empty_row_count_per_column(clean_fr_immo_df)",
   "id": "63785b8725cd4cba",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:34:35.993411Z",
     "start_time": "2025-08-06T13:34:29.804556Z"
    }
   },
   "cell_type": "code",
   "source": "null_col_stats.show(20)",
   "id": "73face0dd0d004b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|      column_name|null_count|nan_count|blank_count|null_symbol_count|total_empty_row_count|total_row_count|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "|   id_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "| date_transaction|         0|        0|          0|                0|                    0|        9141573|\n",
      "|             prix|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      departement|         0|        0|          0|                0|                    0|        9141573|\n",
      "|            ville|         0|        0|          0|                0|                    0|        9141573|\n",
      "|      code_postal|         0|        0|          0|                0|                    0|        9141573|\n",
      "|          adresse|         0|        0|          1|                0|                    1|        9141573|\n",
      "|    type_batiment|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         n_pieces|         0|        0|          0|                0|                    0|        9141573|\n",
      "|surface_habitable|         0|        0|          0|                0|                    0|        9141573|\n",
      "|         latitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "|        longitude|         0|        0|          0|                0|                    0|        9141573|\n",
      "+-----------------+----------+---------+-----------+-----------------+---------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-06T13:42:44.549456Z",
     "start_time": "2025-08-06T13:42:26.397538Z"
    }
   },
   "cell_type": "code",
   "source": "get_duplicated_row_count(clean_fr_immo_df)",
   "id": "a4c6a7e9dc882ad2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate row count: 0\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2bdea439e9e09099"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# creating a geometry column\n",
    "\n"
   ],
   "id": "25189f710b996015"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d30347ec5b67f46"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1d427b8181b3b735"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
