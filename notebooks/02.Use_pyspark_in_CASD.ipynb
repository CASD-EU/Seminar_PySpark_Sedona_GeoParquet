{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 02.Use pyspark in CASD\n",
    "\n",
    "In this tutorial, we will learn:\n",
    "- how to install spark and pyspark in CASD\n",
    "- how to create a spark session\n",
    "- read a csv file\n",
    "- read a parquet file\n",
    "\n",
    "\n",
    "## 1. Install spark and pyspark in CASD\n",
    "\n",
    "As we explained before,\n",
    "- `Apache Spark` is a distributed computation framework written mostly in Scala and Java, runs in a JVM(Java Virtual Machine)\n",
    "- `PySpark` is a Python API for Apache Spark.\n",
    "- PySpark talks to Spark engine via `Py4J`. Your Python code → Py4J(serialized and sent to JVM) → Spark core executes it → results sent back to python\n",
    "\n",
    "> Spark framework installation is essential, without it, pyspark will never work.\n",
    "\n",
    "### 1.1 Install spark framework\n",
    "\n",
    "CASD provides an installation script(`InstallSpark.ps1`) to install the `latest spark framework` and underlying JDK available in CASD.\n",
    "You can find this script in `Bureau->Raccourcis->Spark`.\n",
    "\n",
    "Open a powershell terminal and run the below command\n",
    "\n",
    "```powershell\n",
    "# goto the target folder\n",
    "cd C:\\Users\\Public\\Desktop\\Raccourcis\\Spark\n",
    "\n",
    "# run the installation script\n",
    ".\\InstallSpark.ps1\n",
    "```\n",
    "\n",
    "> If everything works well, this script will install spark in `C:\\Users\\<your-id>\\AppData\\Local\\spark\\spark-3.5.5-bin-hadoop3`. It will also install open-jdk, winutils, and set up your\n",
    "> windows env vars.\n",
    "\n",
    "Now let's check if your spark works or not. Open a new powershell terminal and run the below command\n",
    "\n",
    "```text\n",
    "# check the installed spark version\n",
    "spark-shell --version\n",
    "\n",
    "## it may take few seconds to show the output, be patient\n",
    "# expected output\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.5\n",
    "      /_/\n",
    "\n",
    "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 11.0.2\n",
    "Branch HEAD\n",
    "Compiled by user ubuntu on 2024-08-06T11:36:15Z\n",
    "Revision bb7846dd487f259994fdc69e18e03382e3f64f42\n",
    "Url https://github.com/apache/spark\n",
    "Type --help for more information.\n",
    "```\n",
    "\n",
    "> If you can't see the spark output, contact `service@casd.eu`\n",
    "\n",
    "### 1.2 Install pyspark\n",
    "\n",
    "As we mentioned before, CASD recommends you to create a `python virtual environment` for each for your python project.\n",
    "\n",
    "Suppose we will start a new project called `spark-sedona`, let's create a python virtual environment with this name\n",
    "\n",
    "\n",
    "```powershell\n",
    "# create a python virtual environment\n",
    "conda create --name dspark-sedona python --offline\n",
    "\n",
    "# activate the virtual environment\n",
    "conda activate docs_in_paris\n",
    "\n",
    "# check python version\n",
    "python -V\n",
    "\n",
    "# check installed packages\n",
    "pip list\n",
    "\n",
    "# install pyspark\n",
    "pip install pyspark==3.5.5\n",
    "\n",
    "# check the installed pyspark version\n",
    "pip show pyspark\n",
    "\n",
    "# expected output\n",
    "Name: pyspark\n",
    "Version: 3.5.5\n",
    "Summary: Apache Spark Python API\n",
    "......\n",
    "```\n",
    "\n",
    "> You can notice that we have installed a specific version of pyspark. Because the pyspark version must be the same as the spark framework version. As the output in `section 1.1` is **spark-3.5.5**. So we need to\n",
    "> install pyspark-3.5.5\n",
    "\n",
    "## 2. Create a spark session\n",
    "\n",
    "A `Spark session` is the entry point to Apache Spark. A spark session allows us to interact with Spark’s core engine, no matter if you’re working in Python (PySpark), Scala, Java, or R.\n",
    "\n",
    "\n",
    "It encapsulates:\n",
    "\n",
    "- Cluster connection (or local JVM if local mode)\n",
    "- Configuration settings (memory, partitions, serializer, etc.)\n",
    "- Access to Spark’s APIs: Spark SQL API, DataFrame and Dataset API, RDD API (via .sparkContext), Streaming and machine learning APIs\n",
    "\n",
    "To create a spark session, you need to\n",
    "- import the required module\n",
    "- configure the spark session settings\n",
    "- create the spark session instance\n",
    "\n",
    "### 2.1 A minimum spark session creation\n",
    "\n",
    "Below shows a minimum spark session creation."
   ],
   "id": "b42cf971a64f18ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:42:52.017609Z",
     "start_time": "2025-08-26T14:42:50.806992Z"
    }
   },
   "cell_type": "code",
   "source": "from pyspark.sql import SparkSession, DataFrame",
   "id": "6fbe633fa74cd51c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:42:52.031742Z",
     "start_time": "2025-08-26T14:42:52.027647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"python\""
   ],
   "id": "15e1ce197abfa9d9",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:43:24.488690Z",
     "start_time": "2025-08-26T14:42:52.043526Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# create a spark session in local mode\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"Use_pyspark_in_CASD\") \\\n",
    "    .getOrCreate()"
   ],
   "id": "dbcc286d4b68fd6d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:32.732924Z",
     "start_time": "2025-08-21T13:57:32.697886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# you can get and set configuration of your spark session any moments\n",
    "# get all conf\n",
    "spark.sparkContext.getConf().getAll()"
   ],
   "id": "45405f8a15ae8cea",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.app.id', 'local-1755784652081'),\n",
       " ('spark.app.name', 'Use_pyspark_in_CASD'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.host', 'LT-5CG4181HBL.casd.me'),\n",
       " ('spark.driver.port', '51159'),\n",
       " ('spark.app.startTime', '1755784650515'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.submitTime', '1755784650346'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.2 Check if the spark session works\n",
    "\n",
    "Let's create a dataframe and do some basic operations."
   ],
   "id": "90d0c328eac97e28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:46.530802Z",
     "start_time": "2025-08-21T13:57:32.741931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StringType, StructField, IntegerType\n",
    "\n",
    "# create a dataframe by using List\n",
    "dept = [(\"Alice\", \"Finance\", 10),\n",
    "        (\"Bob\", \"Marketing\", 20),\n",
    "        (\"Charlie\", \"Sales\", 30),\n",
    "        (\"Toto\", \"IT\", 40)\n",
    "        ]\n",
    "\n",
    "# give an explicit schema\n",
    "deptSchema = StructType([\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('dept_name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# create dataframe\n",
    "deptDF1 = spark.createDataFrame(data=dept, schema=deptSchema)\n",
    "deptDF1.printSchema()\n",
    "deptDF1.show(truncate=False)"
   ],
   "id": "f32a5bef0a3d4ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- dept_name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n",
      "+-------+---------+---+\n",
      "|name   |dept_name|age|\n",
      "+-------+---------+---+\n",
      "|Alice  |Finance  |10 |\n",
      "|Bob    |Marketing|20 |\n",
      "|Charlie|Sales    |30 |\n",
      "|Toto   |IT       |40 |\n",
      "+-------+---------+---+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:55.136826Z",
     "start_time": "2025-08-21T13:57:46.553062Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# show the basic stats\n",
    "deptDF1.describe().show()"
   ],
   "id": "c5731669289e14ec",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------+------------------+\n",
      "|summary| name|dept_name|               age|\n",
      "+-------+-----+---------+------------------+\n",
      "|  count|    4|        4|                 4|\n",
      "|   mean| NULL|     NULL|              25.0|\n",
      "| stddev| NULL|     NULL|12.909944487358056|\n",
      "|    min|Alice|  Finance|                10|\n",
      "|    max| Toto|    Sales|                40|\n",
      "+-------+-----+---------+------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2.2 Optimize the spark session config (Optional)\n",
    "\n",
    "When the data volume which you treat is much bigger than your memory, you may need to optimize the spark session config. Here we only give\n",
    "you an example, it may be not suitable for your workflow, because the config depends on your `server configuration` and `data volume`.\n",
    "\n",
    "In general, spark tries to store the data source and intermediate data(shuffle results) in memory. If the memory is not big enough to store the\n",
    "data, we can ask spark to spill the data on hard drive.\n",
    "\n",
    "In the below example, we configure:\n",
    "- how many memory the spark can use\n",
    "- how many memory spark will use for storage and calculation\n",
    "- ETC.\n",
    "\n",
    "> You don't need to understand all, just keep in mind we can optimize spark for large data volume calculation\n"
   ],
   "id": "5963e8a55526eefd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:55.241143Z",
     "start_time": "2025-08-21T13:57:55.219856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"LocalMode_memo_config\")\n",
    "    .master(\"local[*]\")\n",
    "    # JVM memory allocation\n",
    "    .config(\"spark.driver.memory\", \"16g\")  # Half of RAM for driver\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\")  # Avoid OOM on collect()\n",
    "    # Shuffle & partition tuning\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"12\")  # Lower than default 200\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\")  # Avoid large partitions in memory\n",
    "    .config(\"spark.reducer.maxSizeInFlight\", \"48m\")  # Limit shuffle buffer\n",
    "    # Unified memory management\n",
    "    .config(\"spark.memory.fraction\", \"0.7\")  # Reduce pressure on execution memory\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")  # Smaller cache area\n",
    "    .config(\"spark.memory.offHeap.enabled\", \"true\")\n",
    "    .config(\"spark.memory.offHeap.size\", \"1g\")\n",
    "    # Spill to disk early instead of crashing\n",
    "    .config(\"spark.shuffle.spill\", \"true\")\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\")\n",
    "    .config(\"spark.shuffle.compress\", \"true\")\n",
    "    # optimize jvm GC\n",
    "    .config(\"spark.driver.extraJavaOptions\",\n",
    "            \"-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:+HeapDumpOnOutOfMemoryError\")\n",
    "    # Use Kryo serializer\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    # Optional: buffer size for serialization\n",
    "    .config(\"spark.kryoserializer.buffer\", \"64m\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"512m\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "id": "f220ba18441c3e6",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> In this tutorial, we only focus on spark on local mode. CASD also proposes spark on `yarn` and `k8s` mode. For more information, please contact `service@casd.eu`.",
   "id": "27aa649ec04cfd6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Use spark to treat data\n",
    "\n",
    "In this section, we will show the basics of spark\n",
    "- read data (e.g. csv, parquet, etc.)\n",
    "- basic data validation\n",
    "- basic data cleaning\n",
    "- basic data transformation\n",
    "- basic statistics\n",
    "- write data(e.g. csv, parquet, etc.)\n",
    "\n",
    "The data which we will use in this repo is from [kaggle](https://www.kaggle.com/datasets/benoitfavier/immobilier-france). It contains several interesting datasets. In this repo,\n",
    "we use two of them:\n",
    "\n",
    "- transaction.npz: It contains all French real state transactions between 2012 and 2024\n",
    "- transactions_sample.csv: A subset of all transactions\n",
    "\n",
    "### 3.1 Read csv file\n",
    "\n",
    "CSV files are semi-structure data(the column types are not provided), spark offers the `inferSchema` functionality. It works good enough for most of the case, but you need to\n",
    "always check the inferred column types.\n",
    "\n",
    "> The dataset is all france immo transaction records."
   ],
   "id": "6aa98d7f0a569594"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:55.280206Z",
     "start_time": "2025-08-21T13:57:55.276214Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "project_root_dir = Path.cwd().parent\n",
    "data_dir = project_root_dir / \"data\"\n",
    "print(data_dir)"
   ],
   "id": "89718728313d2873",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PLIU\\Documents\\git\\Seminar_PySpark_Sedona_GeoParquet\\data\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:56.542772Z",
     "start_time": "2025-08-21T13:57:55.306931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "csv_immo_file_path = data_dir / \"transactions_sample.csv\"\n",
    "\n",
    "# the option header\n",
    "immo_csv_df = spark.read.csv(csv_immo_file_path.as_posix(), header=True, inferSchema=True)"
   ],
   "id": "1a381ae6fd78177",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:56.875071Z",
     "start_time": "2025-08-21T13:57:56.581813Z"
    }
   },
   "cell_type": "code",
   "source": "immo_csv_df.show(5)",
   "id": "7151e23236639a2e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|id_transaction|date_transaction|    prix|departement|id_ville|               ville|code_postal|             adresse|type_batiment| vefa|n_pieces|surface_habitable|id_parcelle_cadastre|        latitude|       longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|      10160888|      2015-07-22|222500.0|         63|     247|               MUROL|      63790|         5148  COMBE|       Maison|False|       4|              123|      63247000ZO0165|45.5729726213494|2.94997597336221|                 {}|                        {}|                        {}|               {2387}|                     {}|\n",
      "|      10319766|      2024-06-18|218640.0|         73|      15|          LES ALLUES|      73550|45 RUE DU GRAND C...|  Appartement|False|       1|               23|      73015000AB0347|45.3987396582855|6.56760164004644|                {0}|                        {}|                        {}|                   {}|                     {}|\n",
      "|      11545562|      2020-07-23|254950.0|         77|     316|MORET-LOING-ET-OR...|      77250|1 RUE DE LA CROIX...|       Maison|False|       6|              124|      773161700B0305|48.3337616035885|2.78083033196279|                 {}|                        {}|                        {}|                {815}|                     {}|\n",
      "|      13891173|      2022-10-03|380000.0|         94|      81|     VITRY-SUR-SEINE|      94400|11 VOIE VICTOR MASSE|       Maison|False|       3|               93|      94081000AK0189| 48.796988276575|2.37555811653132|                 {}|                        {}|                        {}|                {208}|                     {}|\n",
      "|      11794772|      2018-07-12|258000.0|         77|     284|               MEAUX|      77100|14 RUE PIERRE BON...|       Maison|False|       5|               96|      77284000AW0090|48.9488159934431|2.89173438180231|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:56.916124Z",
     "start_time": "2025-08-21T13:57:56.910655Z"
    }
   },
   "cell_type": "code",
   "source": "immo_csv_df.printSchema()",
   "id": "b764b67fad6f529c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: date (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- departement: integer (nullable = true)\n",
      " |-- id_ville: integer (nullable = true)\n",
      " |-- ville: string (nullable = true)\n",
      " |-- code_postal: integer (nullable = true)\n",
      " |-- adresse: string (nullable = true)\n",
      " |-- type_batiment: string (nullable = true)\n",
      " |-- vefa: string (nullable = true)\n",
      " |-- n_pieces: integer (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- id_parcelle_cadastre: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- surface_dependances: string (nullable = true)\n",
      " |-- surface_locaux_industriels: string (nullable = true)\n",
      " |-- surface_terrains_agricoles: string (nullable = true)\n",
      " |-- surface_terrains_sols: string (nullable = true)\n",
      " |-- surface_terrains_nature: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:58.027138Z",
     "start_time": "2025-08-21T13:57:57.033877Z"
    }
   },
   "cell_type": "code",
   "source": "immo_csv_df.describe().show()",
   "id": "a8bdb3793f87d93f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+---------------+------------------+----------------+-------------+----+------------------+-----------------+--------------------+-----------------+------------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|summary|   id_transaction|              prix|       departement|          id_ville|          ville|       code_postal|         adresse|type_batiment|vefa|          n_pieces|surface_habitable|id_parcelle_cadastre|         latitude|         longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+-------+-----------------+------------------+------------------+------------------+---------------+------------------+----------------+-------------+----+------------------+-----------------+--------------------+-----------------+------------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|  count|              103|               101|               100|               100|            103|               103|             100|          103| 103|               103|              103|                 103|              103|               103|                103|                       103|                       103|                  103|                    103|\n",
      "|   mean|8264591.601941748| 206762.0984158416|             73.95|            212.02|           NULL|56035.087378640776|            NULL|         NULL|NULL| 3.436893203883495|77.20388349514563|            Infinity|45.96255234664196|1.1822048888130572|               NULL|                      NULL|                      NULL|                 NULL|                   NULL|\n",
      "| stddev|4324346.649011455|176291.46750358352|131.73691068323876|164.69285646225512|           NULL| 27231.09458210134|            NULL|         NULL|NULL|1.5318187505580427|37.64393532992464|                NULL|5.567473063397396| 8.608229924347462|               NULL|                      NULL|                      NULL|                 NULL|                   NULL|\n",
      "|    min|            83448|           -1000.0|                 1|                 2|              ?|              1150|       LA CONQUE|            ?|   -|                 0|               23|      01202000AC0425| 4.93971660021603|  -61.106291498063|          {0,0,0,0}|                      {30}|                     {110}|               {1000}|                     {}|\n",
      "|    max|         14868445|         1271260.0|               973|               654|VITRY-SUR-SEINE|             97300|94 IMP DES LILAS|       Maison|True|                 8|              204|      97302000AH0505| 50.6375958115499|  6.85594432028914|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+-------+-----------------+------------------+------------------+------------------+---------------+------------------+----------------+-------------+----+------------------+-----------------+--------------------+-----------------+------------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2 Read a parquet file\n",
    "\n"
   ],
   "id": "15e77e2aea3f0aad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:58.694242Z",
     "start_time": "2025-08-21T13:57:58.175761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fr_immo_transaction_path = data_dir / \"large_ds\" / \"fr_immo_transaction.parquet\"\n",
    "fr_immo_transactions_df = spark.read.parquet(fr_immo_transaction_path.as_posix())"
   ],
   "id": "446ea935a3539360",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:58.741180Z",
     "start_time": "2025-08-21T13:57:58.703771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "required_col = [\"id_transaction\", \"date_transaction\", \"prix\", \"departement\", \"ville\", \"code_postal\", \"adresse\",\n",
    "                \"type_batiment\", \"n_pieces\", \"surface_habitable\", \"latitude\", \"longitude\"]\n",
    "clean_fr_immo_df = fr_immo_transactions_df.select(required_col)"
   ],
   "id": "6fb940ee1aa45c3d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:57:59.744701Z",
     "start_time": "2025-08-21T13:57:58.749188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# cache the dataframe for better performance\n",
    "# clean_fr_immo_df.cache()\n",
    "clean_fr_immo_df.show(5)"
   ],
   "id": "64bf43f61b938f74",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|               ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|        141653|      2014-01-02|197000.0|         01|             TREVOUX|       1600|  6346 MTE DES LILAS|  Appartement|       4|               84|45.9423014034837|4.77069364742062|\n",
      "|        141970|      2014-01-02|157500.0|         01|              VIRIAT|       1440|1369 RTE DE STRAS...|       Maison|       4|              103|46.2364072868351|5.26293493674271|\n",
      "|        139240|      2014-01-02|112000.0|         01|SAINT-JEAN-SUR-VEYLE|       1290|5174  SAINT JEAN ...|       Maison|       3|               78|46.2600767509964| 4.9186124567344|\n",
      "|        146016|      2014-01-02|173020.0|         01|             LAGNIEU|       1150|21 GR GRANDE RUE ...|       Maison|       4|               72|45.8990448750694|5.35421986570434|\n",
      "|        145911|      2014-01-03| 88000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       3|              104|46.2584083170745| 5.6408027525738|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:58:25.583084Z",
     "start_time": "2025-08-21T13:57:59.848083Z"
    }
   },
   "cell_type": "code",
   "source": "clean_fr_immo_df.describe().show()",
   "id": "be397d833de3ae6a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+-----------------+---------+-----------------+--------------------+-------------+------------------+-----------------+-----------------+-----------------+\n",
      "|summary|   id_transaction|              prix|      departement|    ville|      code_postal|             adresse|type_batiment|          n_pieces|surface_habitable|         latitude|        longitude|\n",
      "+-------+-----------------+------------------+-----------------+---------+-----------------+--------------------+-------------+------------------+-----------------+-----------------+-----------------+\n",
      "|  count|          9141573|           9141573|          9141573|  9141573|          9141573|             9141573|      9141573|           9141573|          9141573|          9141573|          9141573|\n",
      "|   mean|7739952.499090474|225329.25414140944|61.77079502619517|     NULL|52936.02979454411|   611.3308270676691|         NULL| 3.524251132709874|82.18102442544625| 46.2699373612654|2.385461679804112|\n",
      "| stddev|4446977.865590328|1663532.0597511497|97.48334644158872|     NULL|27696.00479747301|   1573.440212351751|         NULL|1.5448404739813437|43.38602854939865|6.012010692496585|6.443977622752915|\n",
      "|    min|                1|               0.0|               01|     AAST|             1000|                    |  Appartement|                 0|                0|-21.3862188488928|-63.1520742420684|\n",
      "|    max|         15058573|             3.3E9|              974|ZUYTPEENE|            97490|9Z RUE GABRIEL VI...|       Maison|               112|             7626|  51.082059485914| 55.8292394321594|\n",
      "+-------+-----------------+------------------+-----------------+---------+-----------------+--------------------+-------------+------------------+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:58:26.658491Z",
     "start_time": "2025-08-21T13:58:25.641565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col\n",
    "# get the most expensive house\n",
    "print(clean_fr_immo_df.filter(col(\"prix\")>1E9).show())"
   ],
   "id": "8efe22075dc965d6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+-------+-----------+------------+-----------+--------------------+-------------+--------+-----------------+----------------+-----------------+\n",
      "|id_transaction|date_transaction|   prix|departement|       ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|        longitude|\n",
      "+--------------+----------------+-------+-----------+------------+-----------+--------------------+-------------+--------+-----------------+----------------+-----------------+\n",
      "|       3454911|      2019-02-21| 1.75E9|         29|PLOUGOURVEST|      29400|    7015A  KERVICHEN|       Maison|       4|               97|48.5687995867852|-4.06245022089523|\n",
      "|       3450591|      2019-04-19|2.086E9|         29|  PLOUZEVEDE|      29440|  33  CORNIC AN HENT|       Maison|       5|              132|48.5995097745626|-4.11136334325347|\n",
      "|       8848963|      2014-06-26| 1.72E9|         60|     CHAMBLY|      60230|80 RUE DES MARCHANDS|  Appartement|       2|               88|49.1654699361029| 2.24033276834713|\n",
      "|       8851035|      2014-07-01|  3.3E9|         60|     CHAMBLY|      60230|69 RUE DE L'ANCIE...|       Maison|       5|              128|49.1621804026875| 2.25256736018128|\n",
      "+--------------+----------------+-------+-----------+------------+-----------+--------------------+-------------+--------+-----------------+----------------+-----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T13:58:27.207805Z",
     "start_time": "2025-08-21T13:58:26.682354Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get the biggest house\n",
    "print(clean_fr_immo_df.filter(col(\"n_pieces\")>100).show())"
   ],
   "id": "7ae8ecdff28118a3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+-----------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|      ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+-----------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|      14017595|      2020-12-17|119560.0|         91|    DRAVEIL|      91210|15 ALL DES PASTOU...|       Maison|     109|               61|48.6860409415854|2.43481957002938|\n",
      "|      14413157|      2017-03-31|275000.0|         93|MONTFERMEIL|      93370|    42 AV DES CHENES|       Maison|     112|               93|48.8883232214767|2.56155159324066|\n",
      "+--------------+----------------+--------+-----------+-----------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> You can notice the data quality is not very good, even thought the data is provided by the French government.",
   "id": "54f768fe32c66903"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 Data validation\n",
    "\n",
    "To make sure your statistics make sense, always validate the raw data before you start. In general, we have two categories:\n",
    "- general data validation rules: detect duplication, null value in non-null column, Etc.\n",
    "- domain specific validation rules: column values in acceptable range or list, Etc."
   ],
   "id": "1368f0492d6a0c40"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:00.092889Z",
     "start_time": "2025-08-21T14:00:00.086654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import length\n",
    "def find_bad_immo_rows(df:DataFrame)->DataFrame:\n",
    "    \"\"\"\n",
    "    This function takes an immo dataframe and returns a dataframe where rows contains anomaly values\n",
    "    The price is negative, the house surface is negative, and the postal code is not 5 digits number\n",
    "    \"\"\"\n",
    "    return df.filter((col(\"prix\") <= 0) | (col(\"surface_habitable\") <= 0) | (length(col(\"code_postal\")) != 5))"
   ],
   "id": "72db9b440b27f05b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:00.675467Z",
     "start_time": "2025-08-21T14:00:00.579467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bad_rows = find_bad_immo_rows(immo_csv_df)\n",
    "bad_rows.show()"
   ],
   "id": "8ccd155b3b0d2f77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|id_transaction|date_transaction|    prix|departement|id_ville|               ville|code_postal|             adresse|type_batiment| vefa|n_pieces|surface_habitable|id_parcelle_cadastre|        latitude|       longitude|surface_dependances|surface_locaux_industriels|surface_terrains_agricoles|surface_terrains_sols|surface_terrains_nature|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "|        395925|      2020-08-27| 66500.0|          2|      64|              BELLEU|       2200|10 RUE DE STADTHAGEN|       Maison|False|       3|               62|      02064000AB0555|49.3661606906803| 3.3382068630547|                 {}|                        {}|                        {}|                {198}|                     {}|\n",
      "|         83448|      2019-12-12|167000.0|          1|     368|SAINT-JULIEN-SUR-...|       1540|1753 RTE DES CONT...|       Maison|False|       3|               48|      01368000ZA0058|46.1957149258676|4.93493183748207|                 {}|                        {}|             {16479,26937}|               {1710}|                     {}|\n",
      "|        754907|      2014-02-28|241885.0|          3|     310|               VICHY|       3200|     18 AV DE FRANCE|       Maison|False|       7|              174|      03310000AR0138|46.1185058482893| 3.4299317556618|                 {}|                        {}|                        {}|                {153}|                     {}|\n",
      "|         96894|      2018-01-22|168000.0|          1|     202|             LAGNIEU|       1150|40 RUE DU VIEUX C...|       Maison|False|       4|              101|      01202000AC0425|45.9039290129919|5.34656702688418|                 {}|                        {}|                        {}|                 {70}|                     {}|\n",
      "|       4903918|      2016-07-08| -1000.0|       NULL|    NULL|                   ?|      31500|                NULL|            ?|    -|       1|               38|      31555806AD0499|43.6083995991505|1.45671987390696|                 {}|                        {}|                        {}|                   {}|                     {}|\n",
      "+--------------+----------------+--------+-----------+--------+--------------------+-----------+--------------------+-------------+-----+--------+-----------------+--------------------+----------------+----------------+-------------------+--------------------------+--------------------------+---------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have found the error which we introduced, now let's check the real data\n",
   "id": "17a3dbb010fea16e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:03.110288Z",
     "start_time": "2025-08-21T14:00:02.528098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bad_rows = find_bad_immo_rows(clean_fr_immo_df)\n",
    "print(f\"Total bad rows count: {bad_rows.count()}\")\n",
    "bad_rows.show()"
   ],
   "id": "51566a6391a0e00c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total bad rows count: 604235\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|id_transaction|date_transaction|    prix|departement|               ville|code_postal|             adresse|type_batiment|n_pieces|surface_habitable|        latitude|       longitude|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "|        141653|      2014-01-02|197000.0|         01|             TREVOUX|       1600|  6346 MTE DES LILAS|  Appartement|       4|               84|45.9423014034837|4.77069364742062|\n",
      "|        141970|      2014-01-02|157500.0|         01|              VIRIAT|       1440|1369 RTE DE STRAS...|       Maison|       4|              103|46.2364072868351|5.26293493674271|\n",
      "|        139240|      2014-01-02|112000.0|         01|SAINT-JEAN-SUR-VEYLE|       1290|5174  SAINT JEAN ...|       Maison|       3|               78|46.2600767509964| 4.9186124567344|\n",
      "|        146016|      2014-01-02|173020.0|         01|             LAGNIEU|       1150|21 GR GRANDE RUE ...|       Maison|       4|               72|45.8990448750694|5.35421986570434|\n",
      "|        145911|      2014-01-03| 88000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       3|              104|46.2584083170745| 5.6408027525738|\n",
      "|        145024|      2014-01-03|202500.0|         01|ST ETIENNE-SUR-RE...|       1190|    5112  AU MOIROUX|       Maison|       6|              118|46.3900603987491|5.01770950682101|\n",
      "|        148141|      2014-01-03| 49023.3|         01|               BALAN|       1360|7 LOT AFU DE BARB...|       Maison|       5|              105|45.8321367961343|5.09791701908164|\n",
      "|        145399|      2014-01-03| 68000.0|         01|     BOURG-EN-BRESSE|       1000|10Z RUE EDGAR  QU...|  Appartement|       2|               44|  46.20385904116|5.22569295332627|\n",
      "|        143192|      2014-01-03|156750.0|         01|     BOURG-EN-BRESSE|       1000| 15 RUE DE MONTHOLON|  Appartement|       3|               83|46.2000838066075|5.21083723376748|\n",
      "|        146426|      2014-01-03|163640.0|         01|SAINT-MAURICE-DE-...|       1700|      40 AV DES ILES|  Appartement|       3|               84|45.8232892882996|4.97719901458052|\n",
      "|        141750|      2014-01-03|203000.0|         01|             DAGNEUX|       1120|733C RTE DE STE C...|       Maison|       5|               90|45.8599182503004|5.05532938199608|\n",
      "|        148109|      2014-01-03|126000.0|         01|             OYONNAX|       1100| 29B RUE DE LA FORGE|  Appartement|       4|              111|46.2584083170745| 5.6408027525738|\n",
      "|        146587|      2014-01-03|145000.0|         01|            FARAMANS|       1800|100 CHE DE LA COT...|       Maison|       4|               88|45.8984143194015| 5.1238429973443|\n",
      "|        142324|      2014-01-06|180230.0|         01|SAINT-GENIS-SUR-M...|       1380|   5013  LA TERRASSE|       Maison|       5|              200|46.3005918214866|5.01292534875678|\n",
      "|        147770|      2014-01-06|107000.0|         01|     BOURG-EN-BRESSE|       1000|18 RUE GEORGES GU...|  Appartement|       4|               81|46.1940166503064| 5.2085969704023|\n",
      "|        140120|      2014-01-06|177500.0|         01|BELLEGARDE-SUR-VA...|       1200|    23 RUE DE MUSSEL|       Maison|       6|              120|46.1012359876184|5.81606709511131|\n",
      "|        142088|      2014-01-06|165000.0|         01|BELLEGARDE-SUR-VA...|       1200|        10 RUE VIALA|  Appartement|       4|               87|46.1065010130505|5.82630764368207|\n",
      "|        146625|      2014-01-06|152000.0|         01|           BELLIGNAT|       1100|16 RUE GUSTAVE FL...|       Maison|       5|               92|46.2476079780747|5.63367658432816|\n",
      "|        146683|      2014-01-06|415000.0|         01|                 GEX|       1170|    111 RUE DE PARIS|  Appartement|       5|              117|46.3369095806716|6.05697509698794|\n",
      "|        144187|      2014-01-06|430500.0|         01|           MONTHIEUX|       1390|   300 GR GRANDE RUE|       Maison|       5|              175| 45.954218351011|4.94060008338832|\n",
      "+--------------+----------------+--------+-----------+--------------------+-----------+--------------------+-------------+--------+-----------------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.4 Clean dataframe\n",
    "\n",
    "After we have detected the bad rows, we need to clean them. Even though the best way it to correct the bad values, but in general, we don't have the\n",
    " knowledge to correct them. So we just drop the bad rows:\n",
    "- drop duplicated rows\n",
    "- drop bad value rows\n",
    "- handle mission values(e.g. null, empty strings, NaN, etc.)\n"
   ],
   "id": "b998c95a2e3087e6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:04.227712Z",
     "start_time": "2025-08-21T14:00:04.220400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop duplicates\n",
    "# if no column name is specified, two rows are consider as duplicated, if all column values are equal\n",
    "immo_csv_dedup_df = immo_csv_df.drop_duplicates()"
   ],
   "id": "649df8a50966851b",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:04.956204Z",
     "start_time": "2025-08-21T14:00:04.655899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Total rows count: {immo_csv_df.count()}\")\n",
    "print(f\"Rows count after dedup: {immo_csv_dedup_df.count()}\")"
   ],
   "id": "15c5dfa28326a370",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows count: 103\n",
      "Rows count after dedup: 102\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:05.265854Z",
     "start_time": "2025-08-21T14:00:05.038599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# we can also use a unique value column to accelerate the process\n",
    "# if two rows have the same value at id_transaction column, then two columns are considered duplicated.\n",
    "df_col_dedup = immo_csv_df.drop_duplicates(subset=[\"id_transaction\"])\n",
    "print(f\"Total rows count: {immo_csv_df.count()}\")\n",
    "print(f\"Rows count after dedup: {df_col_dedup.count()}\")"
   ],
   "id": "53ddf2817a9b785e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows count: 103\n",
      "Rows count after dedup: 102\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:06.647252Z",
     "start_time": "2025-08-21T14:00:05.857383Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# get bad row ids\n",
    "bad_row_ids = find_bad_immo_rows(immo_csv_df).select(\"id_transaction\").rdd.flatMap(lambda x: x).collect()\n",
    "print(bad_row_ids)"
   ],
   "id": "63785b8725cd4cba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[395925, 83448, 754907, 96894, 4903918]\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:07.199642Z",
     "start_time": "2025-08-21T14:00:06.689642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# drop all bad rows\n",
    "clean_immo_csv_df = immo_csv_dedup_df.filter(~ col(\"id_transaction\").isin(bad_row_ids))\n",
    "print(f\"Total rows count: {immo_csv_df.count()}\")\n",
    "print(f\"Rows count after drop bad rows: {clean_immo_csv_df.count()}\")"
   ],
   "id": "73face0dd0d004b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows count: 103\n",
      "Rows count after drop bad rows: 97\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.5 Data transformation\n",
    "\n",
    "We often need to transform the raw column into a specific format, so other users can use it directly. In this tutorial, we will do three basic transformation\n",
    "- create a new column `prix_m2`.\n",
    "- extract year and month from `date_transaction` column.\n",
    "- Normalize boolean column `vefa`.\n",
    "\n"
   ],
   "id": "e72333c8541cabc7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:08.225503Z",
     "start_time": "2025-08-21T14:00:07.908458Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import year, month, when\n",
    "\n",
    "immo_csv_df_transformed = clean_immo_csv_df \\\n",
    "    .withColumn(\"prix_m2\", col(\"prix\") / col(\"surface_habitable\")) \\\n",
    "    .withColumn(\"year\", year(col(\"date_transaction\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"date_transaction\"))) \\\n",
    "    .withColumn(\"vefa\", when(col(\"vefa\") == \"True\", True).otherwise(False))\n",
    "\n",
    "immo_csv_sub_df = immo_csv_df_transformed.select(\"id_transaction\",\"date_transaction\",\"prix\",\"surface_habitable\",\"prix_m2\",\"year\",\"month\",\"vefa\")\n",
    "immo_csv_sub_df.show(5)"
   ],
   "id": "3d30347ec5b67f46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------------+--------+-----------------+------------------+----+-----+-----+\n",
      "|id_transaction|date_transaction|    prix|surface_habitable|           prix_m2|year|month| vefa|\n",
      "+--------------+----------------+--------+-----------------+------------------+----+-----+-----+\n",
      "|       4572010|      2019-03-15|181000.0|               72|2513.8888888888887|2019|    3|false|\n",
      "|       5984815|      2023-06-23|288000.0|               60|            4800.0|2023|    6|false|\n",
      "|        964733|      2020-06-11|288600.0|              114|2531.5789473684213|2020|    6|false|\n",
      "|       3390072|      2020-10-12|225000.0|              104|2163.4615384615386|2020|   10|false|\n",
      "|      10621205|      2020-07-29|776000.0|               86| 9023.255813953489|2020|    7| true|\n",
      "+--------------+----------------+--------+-----------------+------------------+----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:08.879527Z",
     "start_time": "2025-08-21T14:00:08.873525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transformed df schema\n",
    "immo_csv_sub_df.printSchema()"
   ],
   "id": "1d427b8181b3b735",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id_transaction: integer (nullable = true)\n",
      " |-- date_transaction: date (nullable = true)\n",
      " |-- prix: double (nullable = true)\n",
      " |-- surface_habitable: integer (nullable = true)\n",
      " |-- prix_m2: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- vefa: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:09.364977Z",
     "start_time": "2025-08-21T14:00:09.353985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# in the raw data schema, column type of vefa is string, not boolean\n",
    "immo_csv_df.select(\"vefa\").printSchema()"
   ],
   "id": "f512eef7919b1b93",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- vefa: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.6 Basic statistics\n",
    "\n",
    "Now our data is ready, let's calculate some basic statistics:\n",
    "- Average price/m2 per department.\n",
    "- Median price per year.\n",
    "- Number of transactions per city."
   ],
   "id": "eb19ed40b2b439e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:10.759014Z",
     "start_time": "2025-08-21T14:00:10.121159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import avg, count, expr\n",
    "\n",
    "# Average price per department\n",
    "avg_dep = immo_csv_df_transformed.groupBy(\"departement\").agg(avg(\"prix_m2\").alias(\"avg_prix\"))\n",
    "avg_dep.orderBy(col(\"avg_prix\").desc()).show()\n"
   ],
   "id": "efd56e8547e5bc9e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|departement|          avg_prix|\n",
      "+-----------+------------------+\n",
      "|         75| 9867.567567567568|\n",
      "|         92| 6145.588595739776|\n",
      "|         94| 5869.356906534325|\n",
      "|         74| 5675.516795865633|\n",
      "|         14|            5000.0|\n",
      "|         69| 4918.683513603144|\n",
      "|         73|  4496.47342995169|\n",
      "|         78| 4199.299450549451|\n",
      "|         95|  3955.94653573377|\n",
      "|         83|  3936.78246441829|\n",
      "|         34|3857.1428571428573|\n",
      "|         29|3560.9756097560976|\n",
      "|         91|3465.7258064516127|\n",
      "|         59|3379.4329896907216|\n",
      "|         44| 3212.128924996572|\n",
      "|         11|           3156.25|\n",
      "|         33|3148.2084555229717|\n",
      "|         56| 2769.230769230769|\n",
      "|         76|2562.7594627594626|\n",
      "|         79|2547.3684210526317|\n",
      "+-----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:11.595854Z",
     "start_time": "2025-08-21T14:00:10.915046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Median price per year (using approx_percentile for performance)\n",
    "median_year = immo_csv_df_transformed.groupBy(\"year\").agg(\n",
    "    expr(\"percentile_approx(prix, 0.5)\").alias(\"median_prix\")\n",
    ")\n",
    "median_year.orderBy(col(\"year\").desc()).show()"
   ],
   "id": "7a3b087ff8297b50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|year|median_prix|\n",
      "+----+-----------+\n",
      "|2024|   218640.0|\n",
      "|2023|   210000.0|\n",
      "|2022|   152000.0|\n",
      "|2021|   135000.0|\n",
      "|2020|   225000.0|\n",
      "|2019|   166000.0|\n",
      "|2018|   216000.0|\n",
      "|2017|   160000.0|\n",
      "|2016|   137000.0|\n",
      "|2015|   170000.0|\n",
      "|2014|   130000.0|\n",
      "+----+-----------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:13.600959Z",
     "start_time": "2025-08-21T14:00:13.045454Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Transactions per city\n",
    "tx_per_city = immo_csv_df_transformed.groupBy(\"ville\").agg(count(\"*\").alias(\"n_transactions\"))\n",
    "tx_per_city.orderBy(col(\"n_transactions\").desc()).show(5)"
   ],
   "id": "b81359ee588cc0bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+\n",
      "|             ville|n_transactions|\n",
      "+------------------+--------------+\n",
      "|            RENNES|             2|\n",
      "|     SAINT RAPHAEL|             2|\n",
      "|            TOULON|             2|\n",
      "|          TOULOUSE|             2|\n",
      "|LA BAULE-ESCOUBLAC|             1|\n",
      "+------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.7 Write data as csv, parquet\n",
    "\n",
    "We have calculated the statistics, now let's write the result in a file"
   ],
   "id": "6309de9a76c7c01a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:15.760057Z",
     "start_time": "2025-08-21T14:00:15.754602Z"
    }
   },
   "cell_type": "code",
   "source": "out_data_path = data_dir / \"tmp\"",
   "id": "e5b797c881ac5bb4",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:17.192786Z",
     "start_time": "2025-08-21T14:00:16.561263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_price_dep_parquet_path = out_data_path / \"avg_price_dep_parquet\"\n",
    "avg_dep.write.mode(\"overwrite\").parquet(avg_price_dep_parquet_path.as_posix())"
   ],
   "id": "8c3bb86e16b2efb6",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-21T14:00:18.217440Z",
     "start_time": "2025-08-21T14:00:17.935696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "avg_price_dep_csv_path = out_data_path / \"avg_price_dep_csv\"\n",
    "avg_dep.write.mode(\"overwrite\").option(\"header\",\"true\").csv(avg_price_dep_csv_path.as_posix())"
   ],
   "id": "81caf585af64a39f",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Close spark session\n",
    "\n",
    "If you don't close the spark session, the reserved resources can be released. So your teammates can't use them."
   ],
   "id": "2e22f9c8ceb6290c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-26T14:46:30.911123Z",
     "start_time": "2025-08-26T14:46:30.405986Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "8e22bd898863cdc0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ea5fb4502a592742"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
