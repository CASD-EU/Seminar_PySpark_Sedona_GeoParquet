<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

    <title>Parquet and GeoParquet</title>

    <link rel="stylesheet" href="dist/reset.css"/>
    <link rel="stylesheet" href="dist/reveal.css"/>
    <link rel="stylesheet" href="dist/theme/dracula.css"/>
    <style>
        .left-align {
            text-align: left;
        }
    </style>
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css"/>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <section>
            <!--Slide 0: overview-->
            <section>
                <a href="https://casd.eu">
                    <img src="assets/CASD.png" alt="casd logo" style="
                  height: 250px;
                  margin: 0 auto 4rem auto;
                  background: transparent;
                  -webkit-filter: invert(1);
                  filter: invert(1);
                " class="demo-logo"/>
                </a>
                <h3>Parquet and GeoParquet</h3>
                <p>
                    <small>Datascience team</small>
                </p>
            </section>

            <section class="left-align">
                <h3>Goals</h3>
                <ul>
                    <li>What is Parquet?</li>
                    <li>How to read/write Parquet file?</li>
                    <li>What is GeoParquet?</li>
                    <li>How to read/write GeoParquet?</li>
                </ul>
            </section>
        </section>

        <!--Slide 1: What is parquet?-->
        <section>
            <section>
                <h3>What is Parquet?</h3>

                <p><em>Apache Parquet</em> is a columnar storage file format widely used in big data analytics (e.g.
                    Spark,
                    Flink, Hive, Dask, DuckDB, etc.).
                </p>

                <p><em>Apache Parquet</em> was launched by Cloudera and Twitter in 2012, top level apache project since
                    2015. Two major versions(v1 and v2) have been released.
                </p>

            </section>

            <!--Slide 1-1: Advantages of Parquet-->
            <section>
                <h3>Advantages of Parquet</h3>
                <ul>
                    <li><em>Storage space efficiency</em>: 5-10 times less storage space compare to CSV, json.
                    </li>
                    <li><em>Analytical query efficiency</em>: 5-10 times faster query time compare to CSV, json.
                    </li>
                    <li><em>Schema evolution</em>: Supports adding new columns without rewriting the entire dataset.
                    </li>
                    <li><em>Vast framework support</em>: Spark, Flink, Pandas, Dask, DuckDB, etc.</li>

                </ul>
            </section>

            <!--Slide 1-2: disadvantages of Parquet-->
            <section>
                <h3>Some disadvantage of Parquet.</h3>
                <ul>
                    <li><em>Not ideal for small datasets</em>: metadata and encoding may cost you more than the actual
                        data
                    </li>
                    <li><em>Not human-readable</em>: Parquet is a binary format, requires tool to read and debug</li>
                    <li><em>Heavy cost on write</em>: Writing parquet file is more CPU-intensive compare to CSV, json
                        (e.g. sorting, encoding, compression)
                    </li>
                    <li><em>Heavy cost on insert</em>: Insert or update a row may require to rewrite the whole parquet
                        file
                    </li>

                </ul>
            </section>
        </section>

        <!--Slide 2: Key concepts in parquet-->
        <section>
            <section>
                <h3>Key concepts in parquet</h3>
                <ul>
                    <li><em>Columnar storage</em>: Parquet organizes data column by column instead of row by row.
                    </li>
                    <li><em>Projection push-down, predicate push-down</em>: Parquet provides column level index to
                        reduce
                        data IO.
                    </li>
                    <li><em>Partitions</em>: Parquet splits data into partitions.</li>
                    <li><em>Integrated schema and metadata</em>: Parquet contains full dataset schema and customizable
                        metadata
                    </li>
                    <li><em>Encoding and compression</em>: Parquet provides column level encoding and compression</li>
                </ul>
            </section>

            <section>
                <h3>Column storage vs Row storage</h3>
                <ul>
                    <li><em>Column oriented(e.g. Parquet, ORC)</em>: Write Once Rea Many(WORM) paradigm, less storage
                        usage, less computing time.
                    </li>
                    <li><em>Row oriented(e.g. CSV, SAS7BDAT)</em>: Easy to insert new rows</li>
                </ul>
                <img src="assets/column_vs_row.png" alt="column_vs_row.png"/>
            </section>
            <section>
                <h3>Parquet file layout example</h3>
                <img src="assets/parquet_file_layout.png" alt="parquet_file_layout.png"/>
            </section>

            <section>
                <h3>Projection pushdown</h3>
                <small><p><em>Projection pushdown</em> is an optimization where the query engine only reads the columns
                    that
                    are actually requested, instead of scanning all columns from a Parquet file.
                </p></small>
                <img src="assets/projection_pushdown.png" alt="projection_pushdown.png"
                     style="width:70%; height:auto;"/>
            </section>

            <section>
                <h3>Predicate pushdown</h3>
                <small><p><em>Predicate pushdown</em> is an optimization where filtering conditions are applied directly
                    while reading Parquet files, rather than loading
                    all the data into memory and then filtering.
                </p></small>
                <img src="assets/predicate_pushdown.png" alt="predicate_pushdown.png" style="width:70%; height:auto;"/>
            </section>

            <section>
                <h3>Partitioning</h3>
                <small><p><em>Partitioning</em> is a data organization technique where a large dataset is split
                    into multiple Parquet files based on the values of one or more columns. Instead of storing all
                    rows in a single file, rows are grouped into folders (directories) according to partition column
                    values.
                </p></small>
                <img src="assets/parquet_partition_example.png" alt="parquet_partition_example.png"
                     style="width:60%; height:auto;"/>
            </section>
            <section>
                <h3>Encoding</h3>
                <small><p><em>Encoding</em> is a technique for representing column values in a more compact way to
                    reduce storage size and improve scan performance. Parquet supports: Dictionary Encoding, Run-Length
                    Encoding (RLE), Bit-Packing (BIT_PACKED), Delta Encoding, etc.
                </p></small>
                <img src="assets/parquet_encoding_example.png" alt="parquet_encoding_example.png"
                     style="width:80%; height:auto;"/>
            </section>

            <section>
                <h3>Compression</h3>
                <small><p><em>Compression</em> in Parquet is applied on data pages (after encoding) to further
                    reduce storage size and speed up I/O.
                    Each Parquet column chunk can use a different compression codec, independent of others.
                </p></small>
                <ul>
                    <li><em>Snappy(Spark default)</em>: Fast compression & decompression, moderate compression ratio.
                    </li>
                    <li><em>Gzip</em>: Higher compression ratio than Snappy, slower read/write performance
                    </li>
                    <li><em>Zstd (Zstandard)</em>: Better compression ratio than Snappy, much faster decompression than Gzip</li>
                    <li><em>LZ4</em>: Very fast compression, lower compression ratio than Zstd or Gzip
                    </li>
                    <li><em>Brotli</em>: Very high compression ratio (better than Gzip in many cases), slower than Snappy/LZ4</li>
                </ul>
            </section>
        </section>

         <!--Slide 3: what is geo parquet-->
        <section>
            <section>
                <h3>What is GeoParquet?</h3>
GeoParquet is an extension of Parquet for geospatial data.
It standardizes how geometries (points, lines, polygons, etc.) are stored inside Parquet files, making them interoperable across tools.

Why GeoParquet?

Parquet doesn’t define how to store geometries.

Geospatial data needs geometry encoding + metadata (CRS, bounding box, geometry type).

GeoParquet ensures compatibility between tools like GeoPandas, Apache Sedona, DuckDB, Spark, PostGIS (via export), and cloud warehouses.

Metadata stored:

Geometry column name (e.g., "geometry").

Encoding (usually WKB – Well-Known Binary).

CRS (Coordinate Reference System, like EPSG:4326).

Bounding box of the dataset.

Geometry type(s) (Point, Polygon, etc.).
            </section>
        </section>


    </div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
    });
</script>
</body>

</html>