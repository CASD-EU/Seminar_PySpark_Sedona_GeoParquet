<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

    <title>Parquet and GeoParquet</title>

    <link rel="stylesheet" href="dist/reset.css"/>
    <link rel="stylesheet" href="dist/reveal.css"/>
    <link rel="stylesheet" href="dist/theme/dracula.css"/>
    <style>
        .left-align {
            text-align: left;
        }
    </style>
    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css"/>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <section>
            <!--Slide 0: overview-->
            <section>
                <a href="https://casd.eu">
                    <img src="assets/CASD.png" alt="casd logo" style="
                  height: 250px;
                  margin: 0 auto 4rem auto;
                  background: transparent;
                  -webkit-filter: invert(1);
                  filter: invert(1);
                " class="demo-logo"/>
                </a>
                <h3>Parquet and GeoParquet</h3>
                <p>
                    <small>Datascience team</small>
                </p>
            </section>

            <section class="left-align">
                <h3>Goals</h3>
                <ul>
                    <li>What is Parquet?</li>
                    <li>How to read/write Parquet file?</li>
                    <li>What is GeoParquet?</li>
                    <li>How to read/write GeoParquet?</li>
                </ul>
            </section>
        </section>

        <!--Slide 1: What is parquet?-->
        <section>
            <section>
                <h3>What is Parquet?</h3>

                <p><em>Apache Parquet</em> is a columnar storage file format widely used in big data analytics (e.g.
                    Spark,
                    Flink, Hive, Dask, DuckDB, etc.).
                </p>

                <p><em>Apache Parquet</em> was launched by Cloudera and Twitter in 2012, top level apache project since
                    2015. Two major versions(v1 and v2) have been released.
                </p>

            </section>

            <!--Slide 1-1: Advantages of Parquet-->
            <section>
                <h3>Advantages of Parquet</h3>
                <ul>
                    <li><em>Storage space efficiency</em>: 5-10 times less storage space compare to CSV, json.
                    </li>
                    <li><em>Analytical query efficiency</em>: 5-10 times faster query time compare to CSV, json.
                    </li>
                    <li><em>Schema evolution</em>: Supports adding new columns without rewriting the entire dataset.
                    </li>
                    <li><em>Vast framework support</em>: Spark, Flink, Pandas, Dask, DuckDB, etc.</li>

                </ul>
            </section>

            <!--Slide 1-2: disadvantages of Parquet-->
            <section>
                <h3>Some disadvantage of Parquet.</h3>
                <ul>
                    <li><em>Not ideal for small datasets</em>: metadata and encoding may cost you more than the actual
                        data
                    </li>
                    <li><em>Not human-readable</em>: Parquet is a binary format, requires tool to read and debug</li>
                    <li><em>Heavy cost on write</em>: Writing parquet file is more CPU-intensive compare to CSV, json
                        (e.g. sorting, encoding, compression)
                    </li>
                    <li><em>Heavy cost on insert</em>: Insert or update a row may require to rewrite the whole parquet
                        file
                    </li>

                </ul>
            </section>
        </section>

        <!--Slide 2: Key concepts in parquet-->
        <section>
            <section>
                <h3>Key concepts in parquet</h3>
                <ul>
                    <li><em>Columnar storage</em>: Parquet organizes data column by column instead of row by row.
                    </li>
                    <li><em>Projection push-down, predicate push-down</em>: Parquet provides column level index to
                        reduce
                        data IO.
                    </li>
                    <li><em>Partitions</em>: Parquet splits data into partitions.</li>
                    <li><em>Integrated schema and metadata</em>: Parquet contains full dataset schema and customizable
                        metadata
                    </li>
                    <li><em>Encoding and compression</em>: Parquet provides column level encoding and compression</li>
                </ul>
            </section>

            <section>
                <h3>Column storage vs Row storage</h3>
                <ul>
                    <li><em>Column oriented(e.g. Parquet, ORC)</em>: Write Once Rea Many(WORM) paradigm, less storage
                        usage, less computing time.
                    </li>
                    <li><em>Row oriented(e.g. CSV, SAS7BDAT)</em>: Easy to insert new rows</li>
                </ul>
                <img src="assets/column_vs_row.png" alt="column_vs_row.png"/>
            </section>
            <section>
                <h3>Parquet file layout example</h3>
                <img src="assets/parquet_file_layout.png" alt="parquet_file_layout.png"/>
            </section>

            <section>
                <h3>Projection pushdown</h3>
                <small><p><em>Projection pushdown</em> is an optimization where the query engine only reads the columns
                    that
                    are actually requested, instead of scanning all columns from a Parquet file.
                </p></small>
                <img src="assets/projection_pushdown.png" alt="projection_pushdown.png"
                     style="width:70%; height:auto;"/>
            </section>

            <section>
                <h3>Predicate pushdown</h3>
                <small><p><em>Predicate pushdown</em> is an optimization where filtering conditions are applied directly
                    while reading Parquet files, rather than loading
                    all the data into memory and then filtering.
                </p></small>
                <img src="assets/predicate_pushdown.png" alt="predicate_pushdown.png" style="width:70%; height:auto;"/>
            </section>

            <section>
                <h3>Partitioning</h3>
                <small><p><em>Partitioning</em> is a data organization technique where a large dataset is split
                    into multiple Parquet files based on the values of one or more columns. Instead of storing all
                    rows in a single file, rows are grouped into folders (directories) according to partition column
                    values.
                </p></small>
                <img src="assets/parquet_partition_example.png" alt="parquet_partition_example.png"
                     style="width:60%; height:auto;"/>
            </section>
            <section>
                <h3>Encoding</h3>
                <small><p><em>Encoding</em> is a technique for representing column values in a more compact way to
                    reduce storage size and improve scan performance. Parquet supports: Dictionary Encoding, Run-Length
                    Encoding (RLE), Bit-Packing (BIT_PACKED), Delta Encoding, etc.
                </p></small>
                <img src="assets/parquet_encoding_example.png" alt="parquet_encoding_example.png"
                     style="width:80%; height:auto;"/>
            </section>

            <section>
                <h3>Compression</h3>
                <small><p><em>Compression</em> in Parquet is applied on data pages (after encoding) to further
                    reduce storage size and speed up I/O.
                    Each Parquet column chunk can use a different compression codec, independent of others.
                </p></small>
                <ul>
                    <small>
                        <li><em>Snappy(Spark default)</em>: Fast compression, moderate compression
                            ratio.
                        </li>
                    </small>
                    <small>
                        <li><em>Gzip</em>: Slower compression, higher compression ratio than Snappy.
                        </li>
                    </small>
                    <small>
                        <li><em>Zstd (Zstandard)</em>: Much faster compression than Gzip, better compression ratio than
                            Snappy.
                        </li>
                    </small>
                    <small>
                        <li><em>LZ4</em>: Very fast compression, lower compression ratio than Zstd or Gzip.
                        </li>
                    </small>
                    <small>
                        <li><em>Brotli</em>: Slower compression than Snappy/LZ4, very high compression ratio (better
                            than Gzip in many
                            cases).
                        </li>
                    </small>
                </ul>
            </section>
        </section>

        <!--Slide 3: what is geo parquet-->
        <section>
            <section>
                <h3>What is GeoParquet?</h3>
                <p>
                    <em>GeoParquet</em> is an extension of Apache Parquet designed for storing <span
                        style="font-weight: bold;">geospatial vector data`
                (points, lines, polygons)</span> in a columnar, compressed, metadata rich format.</p>
                <p>
                    It's already supported by many tools(e.g. Sedona, GeoPandas, QGIS, DuckDB, Kepler GL, etc.)
                    You can visit the <a href="https://geoparquet.org/">GeoParquet official website</a> to learn more.
                </p>
            </section>
            <section>
                <h3>Advantages of GeoParquet</h3>
                <ul>
                    <li class="fragment"><em>Storage space efficiency</em>: columnar storage provides better encoding
                        and
                        compression efficiency.
                    </li>
                    <li class="fragment"><em>Analytical query efficiency</em>: Supports spatial predicate pushdown</li>
                    <li class="fragment"><em>Scalability</em>: Partitioning supports better distributed computing</li>
                    <li class="fragment"><em>Interoperability</em>: Sedona, GeoPandas, QGIS, DuckDB, cloud warehouses
                        (BigQuery, Snowflake, AWS Athena)
                    </li>
                    <li class="fragment"><em>Flexible metadata</em>: CRS, bounding box, geometry type.</li>
                </ul>
            </section>

            <section>
                <h3>Disadvantages of GeoParquet</h3>
                <ul>
                    <li class="fragment"><em>Not human-readable</em>: Binary format requires tools to read the data</li>
                    <li class="fragment"><em>Not popular as old format</em>: shapefile, geojson, etc.</li>
                    <li class="fragment"><em>Don't support raster data</em>: GeoParquet can't host raster data natively
                    </li>
                </ul>
            </section>

        </section>

        <section>
            <section>
                <h3>Key concepts in GeoParquet</h3>

                <small><p>GeoParquet adds <em>geo-encoding (WKB)</em> and <em>geo-metadata</em> (CRS, Geometry types,
                    Bounding
                    box, etc)
                    to Parquet.</p></small>
                <pre><code class="language-json" data-trim data-line-numbers="|1|2|5-8|10-13|">
{
  "version": "1.0.0",
  "primary_column": "geometry",
  "columns": {
    "col1": {
      "encoding": "WKB",
      "geometry_types": ["Polygon", "MultiPolygon"],
      "crs": "EPSG:4326"
    },
    "col2": {
      "encoding": "WKB",
      "geometry_types": ["Point"],
      "crs": "EPSG:3857"
    }
  }
}
                </code></pre>
            </section>

            <section>
                <h3>Bounding Box (bbox) in GeoParquet</h3>

                <p style="font-size: smaller;">
                    The <em>bounding box (bbox)</em> metadata specifies the area covered by geometric
                    shapes in a given file. Sedona can <em>skip that entire file/partition</em>
                    when executing the query because it’s known that it does not cover any relevant data.
                </p>

                 <div style="position: relative; width: 70%; margin: auto;">
    <!-- Invisible reference image to fix container height -->
    <img src="assets/geoparquet_bbox1.png"
         alt=""
         style="width:100%; visibility:hidden; display:block;">

    <!-- First visible image -->
    <img class="fragment fade-out"
         src="assets/geoparquet_bbox1.png"
         alt="GeoParquet bounding box step 1"
         style="width:100%; position:absolute; top:0; left:0;">

    <!-- Second image, revealed after first fades -->
    <img class="fragment fade-in"
         src="assets/geoparquet_bbox2.png"
         alt="GeoParquet bounding box step 2"
         style="width:100%; position:absolute; top:0; left:0;">
  </div>
            </section>

        </section>

    </div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
    });
</script>
</body>

</html>