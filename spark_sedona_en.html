<!DOCTYPE html>
<html lang="en" xmlns="http://www.w3.org/1999/html">

<head>
    <meta charset="utf-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"/>

    <title>Spark and Sedona at CASD</title>

    <link rel="stylesheet" href="dist/reset.css"/>
    <link rel="stylesheet" href="dist/reveal.css"/>
    <link rel="stylesheet" href="dist/theme/dracula.css"/>

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/monokai.css"/>
</head>

<body>
<div class="reveal">
    <div class="slides">
        <!--Slide 1: Overview-->
        <section>
            <section>

                <img src="assets/CASD.png" alt="casd logo" style="
                  height: 250px;
                  margin: 0 auto 4rem auto;
                  background: transparent;
                  -webkit-filter: invert(1);
                  filter: invert(1);
                " class="demo-logo"/>

                <h3>Spark and Sedona at CASD</h3>
                <p>
                    <small>Datascience team</small>
                </p>
            </section>

            <section data-transition="fade-in slide-out">
                <h3>Goals</h3>
                <ul>
                    <li>What is apache spark?</li>
                    <li>Data processing with spark.</li>
                    <li>What is apache sedona?</li>
                    <li>Spatial data processing with sedona.</li>
                </ul>
            </section>
        </section>

        <!--Slide 2: Why use python?-->
        <section data-transition="fade-in slide-out" class="left-align">
            <section data-align="left">
                <h3>Why use Apache Spark?</h3>

                <p>
                    <em>Apache Spark</em> is a <span style="font-weight: bold;">distributed computing framework</span>
                    designed for processing and
                    analyzing large-scale data efficiently. Spark is an <span style="font-weight: bold;">open-source project under the Apache Software
                    Foundation</span>.
                </p>

                <p>
                    <em>Apache Spark</em> is designed to replace the older distributed computing framework <span
                        style="font-weight: bold;">Hadoop</span>.
                    By default, spark performs data computation in memory ont on disk.
                </p>
                <small>Spark is now one of the most widely used big data processing engines.</small>


            </section>
            <section>
                <h4>Spark Key Advantages:</h4>

                <ul>
                    <li class="fragment"><em>Speed</em>: 100 times faster than Hadoop MapReduce</li>
                    <li class="fragment"><em>Versatility</em>: Batch data processing, Machine learning, Graph analytics
                    </li>
                    <li class="fragment"><em>Scalability</em>: Single machine, on-premise clusters, public cloud</li>
                    <li class="fragment"><em>Interoperability</em>: Python, Java, Scala, R and SQL</li>
                    <li class="fragment"><em>Vast and mature ecosystem</em></li>
                </ul>

                <aside class="notes">

                </aside>
            </section>
            <section id="disadvantages">
                <h3>Some disadvantages</h3>
                <ul>
                    <li class="fragment"><em>Requires significant hardware resources</em>: CPU, RAM and network(for
                        cluster mode).
                    </li>
                    <li class="fragment"><em>Requires complex configuration</em>: vast expertise to set up spark cluster
                        correctly.
                    </li>
                    <li class="fragment"><em>Deep learning curve</em>: RDDs, DataFrames, DAG scheduling, partitioning.
                    </li>
                    <li class="fragment"><em>Overhead for Small Jobs</em>: Spark session setup and task scheduling
                        introduce overhead.
                    </li>

                </ul>
            </section>

        </section>

        <!--Slide 3: Key concepts in spark-->
        <section data-transition="fade-in slide-out">
            <!--Slide 3-1: spark general architecture-->
            <section>
                <h3>Key Concepts in spark Application</h3>
                <p>Spark Application contains a <span style="font-weight: bold;">driver program</span> and <span
                        style="font-weight: bold;">data processing tasks</span> on a cluster.
                </p>
                <img src="assets/apache-spark-architecture.png" alt="apache-spark-architecture.png"/>
            </section>
            <!--Slide 3-2: key concept in spark 1-->
            <section>
                <p>Kep concepts in spark(1):</p>

                <ul>

                    <li class="fragment"><em>Driver</em>: Coordinates with the cluster manager and worker, translate
                        tasks into the execution plan, sends tasks to executors.
                    </li>

                    <li class="fragment"><em>Spark Session & Contexts</em>: Entry point of spark application, provides
                        access
                        to DataFrames, SQL queries, and configurations.
                    </li>

                    <li class="fragment"><em>Cluster manager</em>: Allocates CPU and RAM, determines which task runs
                        where and when, monitors tasks and replaces failed ones.
                    </li>

                    <li class="fragment"><em>Executor</em>: Run tasks and store data partitions in memory/disk.
                    </li>

                </ul>
            </section>
            <!--Slide 3-3: key concept in spark 2-->
            <section>
                <p>Kep concepts in spark(2):</p>

                <ul>

                    <li class="fragment"><em>RDD (Resilient Distributed Dataset)</em>: Core data model in spark.
                        A <span style="font-weight: bold;">distributed fault tolerance</span> collection of immutable
                        objects.
                    </li>

                    <li class="fragment"><em>DataFrame</em>: An overlay of RDD, provides sql like operations and
                        optimization(Catalyst, Tungsten)
                    </li>

                    <li class="fragment"><em>Dataset</em>: Type-safe version of DataFrame, only available for Scala/Java
                        API.
                    </li>

                    <li class="fragment"><em>Partitioning</em>: Data(RDD, DataFrame, Dataset) is split into partitions
                        distributed across executors.
                        API.
                    </li>

                </ul>
            </section>

            <!--Slide 3-4: key concept in spark 3-->
            <section>
                <p>Kep concepts in spark(3):</p>

                <ul>

                    <li class="fragment"><em>Driver program</em>: Directed acyclic graph (DAG) of transformations and
                        actions.
                    </li>

                    <li class="fragment"><em>Transformations</em>: map, filter, groupBy, etc.
                    </li>

                    <li class="fragment"><em>Actions</em>: trigger the execution of the transformations before.
                    </li>

                    <li class="fragment"><em>Lazy Evaluation</em>: Transformations are not executed immediately.
                        Execution happens only when an action is called.
                        Allows Spark to optimize the DAG as much as possible.
                    </li>

                </ul>
            </section>

            <!--Slide 3-4: key concept in spark 3-->
            <section>
                <p>Kep concepts in spark(advance):</p>

                <small><p><em>Data shuffling</em> is the redistribution of data across executors. It's one of the most
                    expensive operations, <em>minimizing shuffle is the key for performance.</em>
                </p></small>
                <img src="assets/spark_shuffle.png" alt="spark_shuffle.png"/>

            </section>
        </section>


        <!--Slide 4: spark in CASD-->
        <section data-transition="fade-in slide-out">
            <!-- Slide 4-1: overview      -->
            <section>
                <p>Spark in CASD</p>

                <ul>
                    <li class="fragment"><em>Spark cluster mode</em>: local, and yarn with hdfs</li>
                    <li class="fragment"><em>Spark client API</em>: Python, Java, Scala, R, SQL</li>
                    <li class="fragment"><em>Integrated Development Environment (IDE)</em>: vs-code, r-studio, jupyter
                        notebook
                    </li>
                    <li class="fragment"><em>Project Structure & Configuration Files</em>: CASD best practices</li>
                </ul>
            </section>

            <section>
                <h3>Install spark in CASD</h3>


            </section>
        </section>


        <!--Slide 5: Sedona introduction-->
        <section>
            <h2>Apache Sedona</h2>
            <p><em>Apache Sedona</em> is a cluster computing system for processing large-scale spatial data.
                Sedona extends existing cluster computing systems, such as <span style="font-weight: bold;">Apache Spark, Apache Flink, and Snowflake</span>,
                with a set of out-of-the-box <span style="font-weight: bold;">distributed Spatial Datasets and Spatial SQL</span> that efficiently
                load, process, and analyze large-scale spatial data across machines.</p>

            <p>CASD provides <em>Apache Spark</em> cluster for Sedona.</p>


        </section>


    </div>
</div>

<script src="dist/reveal.js"></script>
<script src="plugin/notes/notes.js"></script>
<script src="plugin/markdown/markdown.js"></script>
<script src="plugin/highlight/highlight.js"></script>
<script>
    // More info about initialization & config:
    // - https://revealjs.com/initialization/
    // - https://revealjs.com/config/
    Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
    });
</script>
</body>

</html>